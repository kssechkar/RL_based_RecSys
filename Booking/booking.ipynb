{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dCs22KpWJ_y"
      },
      "outputs": [],
      "source": [
        "# merge_consec_bookings(initial/BookingMultiDestinations) -> 'processed/my-df-mdt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GikGRPN0WJ_1",
        "outputId": "a9351006-07ed-4642-9b97-01bba8b7c018"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utrip_id</th>\n",
              "      <th>checkin</th>\n",
              "      <th>checkout</th>\n",
              "      <th>city_id</th>\n",
              "      <th>is_buy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-21</td>\n",
              "      <td>2016-07-23</td>\n",
              "      <td>56430</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-23</td>\n",
              "      <td>2016-07-25</td>\n",
              "      <td>41971</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-25</td>\n",
              "      <td>2016-07-28</td>\n",
              "      <td>5797</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>2016-02-08</td>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>50075</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>2016-02-10</td>\n",
              "      <td>29207</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287115</th>\n",
              "      <td>999911_1</td>\n",
              "      <td>2016-10-03</td>\n",
              "      <td>2016-10-05</td>\n",
              "      <td>23921</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287116</th>\n",
              "      <td>999911_1</td>\n",
              "      <td>2016-10-05</td>\n",
              "      <td>2016-10-07</td>\n",
              "      <td>21730</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287117</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-15</td>\n",
              "      <td>2016-08-17</td>\n",
              "      <td>29770</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287118</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-18</td>\n",
              "      <td>2016-08-19</td>\n",
              "      <td>36170</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287119</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-19</td>\n",
              "      <td>2016-08-20</td>\n",
              "      <td>52155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287120 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         utrip_id     checkin    checkout  city_id  is_buy\n",
              "0       1000066_2  2016-07-21  2016-07-23    56430       0\n",
              "1       1000066_2  2016-07-23  2016-07-25    41971       1\n",
              "2       1000066_2  2016-07-25  2016-07-28     5797       1\n",
              "3       1000270_1  2016-02-08  2016-02-09    50075       1\n",
              "4       1000270_1  2016-02-09  2016-02-10    29207       0\n",
              "...           ...         ...         ...      ...     ...\n",
              "287115   999911_1  2016-10-03  2016-10-05    23921       1\n",
              "287116   999911_1  2016-10-05  2016-10-07    21730       1\n",
              "287117   999991_3  2016-08-15  2016-08-17    29770       1\n",
              "287118   999991_3  2016-08-18  2016-08-19    36170       1\n",
              "287119   999991_3  2016-08-19  2016-08-20    52155       0\n",
              "\n",
              "[287120 rows x 5 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from Modules.data_processing import set_rewards_toppop\n",
        "\n",
        "\n",
        "set_rewards_toppop('data/processed/my-df-mdt.csv', save_name='data/processed/my-pop-mdt-buy.csv')\n",
        "set_rewards_toppop('data/processed/test-my-df-mdt.csv', save_name='data/processed/test-my-pop-mdt-buy.csv', pop_ratio=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJjTEc-JWJ_2",
        "outputId": "d4de31e6-6d64-4eea-e244-5bc200073c44"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utrip_id</th>\n",
              "      <th>checkin</th>\n",
              "      <th>checkout</th>\n",
              "      <th>city_id</th>\n",
              "      <th>time_at</th>\n",
              "      <th>is_buy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-21</td>\n",
              "      <td>2016-07-23</td>\n",
              "      <td>56430</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-23</td>\n",
              "      <td>2016-07-25</td>\n",
              "      <td>41971</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-25</td>\n",
              "      <td>2016-07-28</td>\n",
              "      <td>5797</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>2016-02-08</td>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>50075</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>2016-02-10</td>\n",
              "      <td>29207</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287115</th>\n",
              "      <td>999911_1</td>\n",
              "      <td>2016-10-03</td>\n",
              "      <td>2016-10-05</td>\n",
              "      <td>23921</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287116</th>\n",
              "      <td>999911_1</td>\n",
              "      <td>2016-10-05</td>\n",
              "      <td>2016-10-07</td>\n",
              "      <td>21730</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287117</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-15</td>\n",
              "      <td>2016-08-17</td>\n",
              "      <td>29770</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287118</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-18</td>\n",
              "      <td>2016-08-19</td>\n",
              "      <td>36170</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287119</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-19</td>\n",
              "      <td>2016-08-20</td>\n",
              "      <td>52155</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287120 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         utrip_id    checkin   checkout  city_id  time_at  is_buy\n",
              "0       1000066_2 2016-07-21 2016-07-23    56430        2       0\n",
              "1       1000066_2 2016-07-23 2016-07-25    41971        2       0\n",
              "2       1000066_2 2016-07-25 2016-07-28     5797        3       1\n",
              "3       1000270_1 2016-02-08 2016-02-09    50075        1       0\n",
              "4       1000270_1 2016-02-09 2016-02-10    29207        1       0\n",
              "...           ...        ...        ...      ...      ...     ...\n",
              "287115   999911_1 2016-10-03 2016-10-05    23921        2       1\n",
              "287116   999911_1 2016-10-05 2016-10-07    21730        2       0\n",
              "287117   999991_3 2016-08-15 2016-08-17    29770        2       1\n",
              "287118   999991_3 2016-08-18 2016-08-19    36170        1       0\n",
              "287119   999991_3 2016-08-19 2016-08-20    52155        1       0\n",
              "\n",
              "[287120 rows x 6 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from Modules.data_processing import set_rewards_most_time\n",
        "\n",
        "\n",
        "set_rewards_most_time('data/processed/my-df-mdt.csv', save_name='data/processed/my-time-mdt-buy.csv')\n",
        "set_rewards_most_time('data/processed/test-my-df-mdt.csv', save_name='data/processed/test-my-time-mdt-buy.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkuE4ny-WJ_3",
        "outputId": "46a334f3-22c8-499d-d829-e160758a65da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utrip_id</th>\n",
              "      <th>checkin</th>\n",
              "      <th>checkout</th>\n",
              "      <th>city_id</th>\n",
              "      <th>next_city_id</th>\n",
              "      <th>is_buy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-21</td>\n",
              "      <td>2016-07-23</td>\n",
              "      <td>56430</td>\n",
              "      <td>41971.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-23</td>\n",
              "      <td>2016-07-25</td>\n",
              "      <td>41971</td>\n",
              "      <td>5797.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>2016-07-25</td>\n",
              "      <td>2016-07-28</td>\n",
              "      <td>5797</td>\n",
              "      <td>41971.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>2016-02-08</td>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>50075</td>\n",
              "      <td>41685.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>2016-02-10</td>\n",
              "      <td>29207</td>\n",
              "      <td>17128.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287115</th>\n",
              "      <td>999911_1</td>\n",
              "      <td>2016-10-03</td>\n",
              "      <td>2016-10-05</td>\n",
              "      <td>23921</td>\n",
              "      <td>8750.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287116</th>\n",
              "      <td>999911_1</td>\n",
              "      <td>2016-10-05</td>\n",
              "      <td>2016-10-07</td>\n",
              "      <td>21730</td>\n",
              "      <td>5325.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287117</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-15</td>\n",
              "      <td>2016-08-17</td>\n",
              "      <td>29770</td>\n",
              "      <td>55196.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287118</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-18</td>\n",
              "      <td>2016-08-19</td>\n",
              "      <td>36170</td>\n",
              "      <td>19444.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287119</th>\n",
              "      <td>999991_3</td>\n",
              "      <td>2016-08-19</td>\n",
              "      <td>2016-08-20</td>\n",
              "      <td>52155</td>\n",
              "      <td>11633.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287120 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         utrip_id     checkin    checkout  city_id  next_city_id  is_buy\n",
              "0       1000066_2  2016-07-21  2016-07-23    56430       41971.0       0\n",
              "1       1000066_2  2016-07-23  2016-07-25    41971        5797.0       1\n",
              "2       1000066_2  2016-07-25  2016-07-28     5797       41971.0       1\n",
              "3       1000270_1  2016-02-08  2016-02-09    50075       41685.0       0\n",
              "4       1000270_1  2016-02-09  2016-02-10    29207       17128.0       0\n",
              "...           ...         ...         ...      ...           ...     ...\n",
              "287115   999911_1  2016-10-03  2016-10-05    23921        8750.0       0\n",
              "287116   999911_1  2016-10-05  2016-10-07    21730        5325.0       0\n",
              "287117   999991_3  2016-08-15  2016-08-17    29770       55196.0       0\n",
              "287118   999991_3  2016-08-18  2016-08-19    36170       19444.0       0\n",
              "287119   999991_3  2016-08-19  2016-08-20    52155       11633.0       0\n",
              "\n",
              "[287120 rows x 6 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from Modules.data_processing import set_rewards_consecutive\n",
        "\n",
        "\n",
        "set_rewards_consecutive('data/processed/my-df-mdt.csv', save_name='data/processed/my-consec-mdt-buy.csv')\n",
        "set_rewards_consecutive('data/processed/test-my-df-mdt.csv', save_name='data/processed/test-my-consec-mdt-buy.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VBUA3WFdtaNd"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IlOmTGJDWJ_3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h1TN5LxjQXL",
        "outputId": "94367da0-d89e-4c68-8fc1-71570de60194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c59nWQFPjU-6"
      },
      "outputs": [],
      "source": [
        "dat = '/content/drive/MyDrive/COURSEW/BOOKING/data/processed/'  # for collab\n",
        "# dat = 'data/processed/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hczA1Ugs5Uvm"
      },
      "outputs": [],
      "source": [
        "ground_truth = pd.read_csv('/content/drive/MyDrive/COURSEW/BOOKING/data/initial/ground_truth.csv').drop(['id'], axis=1)\n",
        "ground_truth_t = ground_truth.copy()\n",
        "ground_truth_p = ground_truth.copy()\n",
        "ground_truth_c = ground_truth.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "78FGjsztgJd6"
      },
      "outputs": [],
      "source": [
        "from Modules.data_processing import preprocess_booking, get_statistics, create_pop_dict, shorten_sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjEkXFFx5ryP"
      },
      "source": [
        "Processing for the \"most time at\" reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PzLObOWgJd6",
        "outputId": "42820839-7a93-487f-8452-2e0aaf35f0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique: 22872 min: 0 max: 22871\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train_t, itm2idx_t, idx2itm = preprocess_booking(dat + 'my-time-mdt-buy.csv')\n",
        "df_test_t, _, _  = preprocess_booking(dat + 'test-my-time-mdt-buy.csv', itm2idx=itm2idx_t)\n",
        "\n",
        "\n",
        "sh_df_train_t, new_itm2idx_t = shorten_sessions(df_train_t, n_sessions=50000)\n",
        "sh_df_test_t = df_test_t.loc[df_test_t['item_id'].isin(new_itm2idx_t.keys())]\n",
        "sh_df_test_t.loc[:, 'item_id'] = sh_df_test_t['item_id'].map(new_itm2idx_t)\n",
        "sh_df_test_t['item_id'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nqLaHmSk5WEL"
      },
      "outputs": [],
      "source": [
        "ground_truth_t.loc[:, 'city_id'] = ground_truth_t['city_id'].map(itm2idx_t)\n",
        "sh_ground_truth_t = ground_truth_t.loc[ground_truth_t['city_id'].isin(new_itm2idx_t.keys())]\n",
        "sh_ground_truth_t.loc[:, 'city_id'] = sh_ground_truth_t['city_id'].map(new_itm2idx_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErEZr6Pw51-D"
      },
      "source": [
        "Processing for the \"top popular\" reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkysuJRKWJ_7",
        "outputId": "6d08d3ca-3df9-4f9c-ee42-8d8dae43d739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique: 22699 min: 0 max: 22698\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train_p, itm2idx_p, _ = preprocess_booking(dat + 'my-pop-mdt-buy.csv', col_n=5)\n",
        "df_test_p, _, _  = preprocess_booking(dat + 'test-my-pop-mdt-buy.csv', itm2idx=itm2idx_p, col_n=5)\n",
        "\n",
        "\n",
        "sh_df_train_p, new_itm2idx_p = shorten_sessions(df_train_t, n_sessions=50000)\n",
        "sh_df_test_p = df_test_p.loc[df_test_p['item_id'].isin(new_itm2idx_p.keys())]\n",
        "sh_df_test_p.loc[:, 'item_id'] = sh_df_test_p['item_id'].map(new_itm2idx_p)\n",
        "sh_df_test_p['item_id'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YO41JRap57Wj"
      },
      "outputs": [],
      "source": [
        "ground_truth_p.loc[:, 'city_id'] = ground_truth_p['city_id'].map(itm2idx_p)\n",
        "sh_ground_truth_p = ground_truth_p.loc[ground_truth_p['city_id'].isin(new_itm2idx_p.keys())]\n",
        "sh_ground_truth_p.loc[:, 'city_id'] = sh_ground_truth_p['city_id'].map(new_itm2idx_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBAbL-gW6CXy"
      },
      "source": [
        "Processing for the popular \"consecutive cities\" reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRsv086vWJ_7",
        "outputId": "6e6094b6-59c4-4c93-bcf7-4aca046f6e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique: 22782 min: 0 max: 22781\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train_c, itm2idx_c, _ = preprocess_booking(dat + 'my-consec-mdt-buy.csv')\n",
        "df_test_c, _, _  = preprocess_booking(dat + 'test-my-consec-mdt-buy.csv', itm2idx=itm2idx_c)\n",
        "\n",
        "\n",
        "sh_df_train_c, new_itm2idx_c = shorten_sessions(df_train_c, n_sessions=50000)\n",
        "sh_df_test_c = df_test_c.loc[df_test_c['item_id'].isin(new_itm2idx_c.keys())]\n",
        "sh_df_test_c.loc[:, 'item_id'] = sh_df_test_c['item_id'].map(new_itm2idx_c)\n",
        "sh_df_test_c['item_id'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s4mstDKQ6Ltt"
      },
      "outputs": [],
      "source": [
        "ground_truth_c.loc[:, 'city_id'] = ground_truth_c['city_id'].map(itm2idx_c)\n",
        "sh_ground_truth_c = ground_truth_c.loc[ground_truth_c['city_id'].isin(new_itm2idx_c.keys())]\n",
        "sh_ground_truth_c.loc[:, 'city_id'] = sh_ground_truth_c['city_id'].map(new_itm2idx_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FDo0F3j0uHrl"
      },
      "outputs": [],
      "source": [
        "# df_t = pd.concat([df_train_t, df_test_t], ignore_index=True).sort_values(by=['session_id', 'timestamp'])\n",
        "sh_df_t = pd.concat([sh_df_train_t, sh_df_test_t], ignore_index=True).sort_values(by=['session_id', 'timestamp'])\n",
        "# df_p = pd.concat([df_train_p, df_test_p], ignore_index=True).sort_values(by=['session_id', 'timestamp'])\n",
        "sh_df_p = pd.concat([sh_df_train_p, sh_df_test_p], ignore_index=True).sort_values(by=['session_id', 'timestamp'])\n",
        "# df_c = pd.concat([df_train_c, df_test_c], ignore_index=True).sort_values(by=['session_id', 'timestamp'])\n",
        "sh_df_c = pd.concat([sh_df_train_c, sh_df_test_c], ignore_index=True).sort_values(by=['session_id', 'timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "tXPhFlnDWJ_7",
        "outputId": "d217200a-da17-4a44-f312-ed79479e4921"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.hist(sh_df_t.session_id.value_counts())\n",
        "plt.title('Distribution of trip lengths')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbstG_W-4-0d"
      },
      "source": [
        "A good value for history length according to the histogram is 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aM9BabKSlbbr"
      },
      "outputs": [],
      "source": [
        "# replay_buf_t, data_stats_t = get_statistics(df_t, df_train_t, length=7)\n",
        "# replay_buf_p, data_stats_p = get_statistics(df_p, df_train_p, length=7)\n",
        "# replay_buf_c, data_stats_c = get_statistics(df_c, df_train_c, length=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7fcjuu4YvpeY"
      },
      "outputs": [],
      "source": [
        "sh_replay_buf_t, sh_data_stats_t = get_statistics(sh_df_t, sh_df_train_t, length=7)\n",
        "sh_replay_buf_p, sh_data_stats_p = get_statistics(sh_df_p, sh_df_train_p, length=7)\n",
        "sh_replay_buf_c, sh_data_stats_c = get_statistics(sh_df_c, sh_df_train_c, length=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Qp367FZybq",
        "outputId": "27c3827f-7515-4c47-9129-77f0bd3d03de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting trfl\n",
            "  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trfl) (1.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from trfl) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trfl) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trfl) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from trfl) (1.14.1)\n",
            "Installing collected packages: trfl\n",
            "Successfully installed trfl-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# %pip install trfl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wMmpm0-uo7lN"
      },
      "outputs": [],
      "source": [
        "from Modules.training import train_no_eval, predict_booking_batched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lrew_9dbWJ_8"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy_at(submission,ground_truth, at=4):\n",
        "    '''checks if the true city is within the four recommended cities'''\n",
        "    data = submission.merge(ground_truth,on='utrip_id')\n",
        "\n",
        "    if at==4:\n",
        "      hits = ((data['city_id']==data['city_id_1'])|(data['city_id']==data['city_id_2'])|\n",
        "        (data['city_id']==data['city_id_3'])|(data['city_id']==data['city_id_4']))*1\n",
        "    elif at==10:\n",
        "      hits = ((data['city_id']==data['city_id_1'])|(data['city_id']==data['city_id_2'])|\n",
        "        (data['city_id']==data['city_id_3'])|(data['city_id']==data['city_id_4'])|\n",
        "        (data['city_id']==data['city_id_5'])|(data['city_id']==data['city_id_6'])|\n",
        "        (data['city_id']==data['city_id_7'])|(data['city_id']==data['city_id_8'])|\n",
        "        (data['city_id']==data['city_id_9'])|(data['city_id']==data['city_id_10']))*1\n",
        "    return hits.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S62B6olWJ_8"
      },
      "source": [
        "### Most time reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vf0QBo0WJ_9",
        "outputId": "9cae8df4-1fd0-48a9-dcd2-73dc124e1a10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "/content/Modules/Network.py:86: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.seq = tf.compat.v1.layers.dropout(self.seq,\n",
            "/content/Modules/SASRec.py:131: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
            "/content/Modules/SASRec.py:132: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Modules/SASRec.py:133: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Modules/SASRec.py:173: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/SASRec.py:212: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Modules/SASRec.py:213: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/SASRec.py:217: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Modules/SASRec.py:218: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/Network.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Modules/Network.py:117: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$$$$$$$ STARTING EPOCH 0 $$$$$$$\n",
            "the loss in 50th batch / 485 is: 15.692991\n",
            "the loss in 100th batch / 485 is: 10.956091\n",
            "the loss in 150th batch / 485 is: 9.692581\n",
            "the loss in 200th batch / 485 is: 9.696996\n",
            "the loss in 250th batch / 485 is: 9.629321\n",
            "the loss in 300th batch / 485 is: 9.663274\n",
            "the loss in 350th batch / 485 is: 9.450041\n",
            "the loss in 400th batch / 485 is: 9.426473\n",
            "the loss in 450th batch / 485 is: 9.357160\n",
            "$$$$$$$ STARTING EPOCH 1 $$$$$$$\n",
            "the loss in 500th batch / 485 is: 8.558138\n",
            "the loss in 550th batch / 485 is: 8.129884\n",
            "the loss in 600th batch / 485 is: 7.824142\n",
            "the loss in 650th batch / 485 is: 7.662718\n",
            "the loss in 700th batch / 485 is: 7.184328\n",
            "the loss in 750th batch / 485 is: 7.226996\n",
            "the loss in 800th batch / 485 is: 6.918771\n",
            "the loss in 850th batch / 485 is: 6.632276\n",
            "the loss in 900th batch / 485 is: 6.547918\n",
            "the loss in 950th batch / 485 is: 6.508663\n",
            "$$$$$$$ STARTING EPOCH 2 $$$$$$$\n",
            "the loss in 1000th batch / 485 is: 6.637822\n",
            "the loss in 1050th batch / 485 is: 6.311459\n",
            "the loss in 1100th batch / 485 is: 6.548055\n",
            "the loss in 1150th batch / 485 is: 6.273757\n",
            "the loss in 1200th batch / 485 is: 6.287319\n",
            "the loss in 1250th batch / 485 is: 5.893635\n",
            "the loss in 1300th batch / 485 is: 6.205616\n",
            "the loss in 1350th batch / 485 is: 5.805517\n",
            "the loss in 1400th batch / 485 is: 6.314944\n",
            "the loss in 1450th batch / 485 is: 5.868307\n",
            "$$$$$$$ STARTING EPOCH 3 $$$$$$$\n",
            "the loss in 1500th batch / 485 is: 6.015184\n",
            "the loss in 1550th batch / 485 is: 6.007379\n",
            "the loss in 1600th batch / 485 is: 5.854111\n",
            "the loss in 1650th batch / 485 is: 5.715492\n",
            "the loss in 1700th batch / 485 is: 5.761723\n",
            "the loss in 1750th batch / 485 is: 5.711968\n",
            "the loss in 1800th batch / 485 is: 5.651595\n",
            "the loss in 1850th batch / 485 is: 5.688529\n",
            "the loss in 1900th batch / 485 is: 5.459539\n",
            "$$$$$$$ STARTING EPOCH 4 $$$$$$$\n",
            "the loss in 1950th batch / 485 is: 5.408614\n",
            "the loss in 2000th batch / 485 is: 5.708941\n",
            "the loss in 2050th batch / 485 is: 5.231175\n",
            "the loss in 2100th batch / 485 is: 5.592794\n",
            "the loss in 2150th batch / 485 is: 5.586151\n",
            "the loss in 2200th batch / 485 is: 5.650912\n",
            "the loss in 2250th batch / 485 is: 5.589479\n",
            "the loss in 2300th batch / 485 is: 5.512082\n",
            "the loss in 2350th batch / 485 is: 5.190722\n",
            "the loss in 2400th batch / 485 is: 5.204379\n",
            "$$$$$$$ STARTING EPOCH 5 $$$$$$$\n",
            "the loss in 2450th batch / 485 is: 5.137639\n",
            "the loss in 2500th batch / 485 is: 5.126057\n",
            "the loss in 2550th batch / 485 is: 5.231137\n",
            "the loss in 2600th batch / 485 is: 5.070407\n",
            "the loss in 2650th batch / 485 is: 4.962172\n",
            "the loss in 2700th batch / 485 is: 5.114583\n",
            "the loss in 2750th batch / 485 is: 5.285078\n",
            "the loss in 2800th batch / 485 is: 5.124795\n",
            "the loss in 2850th batch / 485 is: 5.038731\n",
            "the loss in 2900th batch / 485 is: 5.195314\n",
            "$$$$$$$ STARTING EPOCH 6 $$$$$$$\n",
            "the loss in 2950th batch / 485 is: 5.072236\n",
            "the loss in 3000th batch / 485 is: 5.019699\n",
            "the loss in 3050th batch / 485 is: 4.904846\n",
            "the loss in 3100th batch / 485 is: 5.003059\n",
            "the loss in 3150th batch / 485 is: 4.940851\n",
            "the loss in 3200th batch / 485 is: 4.855107\n",
            "the loss in 3250th batch / 485 is: 4.900009\n",
            "the loss in 3300th batch / 485 is: 4.918984\n",
            "the loss in 3350th batch / 485 is: 4.959708\n",
            "$$$$$$$ STARTING EPOCH 7 $$$$$$$\n",
            "the loss in 3400th batch / 485 is: 4.879598\n",
            "the loss in 3450th batch / 485 is: 5.043497\n",
            "the loss in 3500th batch / 485 is: 4.555891\n",
            "the loss in 3550th batch / 485 is: 5.072276\n",
            "the loss in 3600th batch / 485 is: 4.888694\n",
            "the loss in 3650th batch / 485 is: 4.960617\n",
            "the loss in 3700th batch / 485 is: 4.844327\n",
            "the loss in 3750th batch / 485 is: 4.911467\n",
            "the loss in 3800th batch / 485 is: 4.806511\n",
            "the loss in 3850th batch / 485 is: 4.707912\n",
            "$$$$$$$ STARTING EPOCH 8 $$$$$$$\n",
            "the loss in 3900th batch / 485 is: 4.749364\n",
            "the loss in 3950th batch / 485 is: 4.914200\n",
            "the loss in 4000th batch / 485 is: 4.865249\n",
            "the loss in 4050th batch / 485 is: 4.651722\n",
            "the loss in 4100th batch / 485 is: 4.585403\n",
            "the loss in 4150th batch / 485 is: 4.762615\n",
            "the loss in 4200th batch / 485 is: 4.650728\n",
            "the loss in 4250th batch / 485 is: 4.781830\n",
            "the loss in 4300th batch / 485 is: 4.951169\n",
            "the loss in 4350th batch / 485 is: 4.961264\n",
            "$$$$$$$ STARTING EPOCH 9 $$$$$$$\n",
            "the loss in 4400th batch / 485 is: 4.644346\n",
            "the loss in 4450th batch / 485 is: 4.588410\n",
            "the loss in 4500th batch / 485 is: 4.697522\n",
            "the loss in 4550th batch / 485 is: 4.369625\n",
            "the loss in 4600th batch / 485 is: 4.480904\n",
            "the loss in 4650th batch / 485 is: 4.563937\n",
            "the loss in 4700th batch / 485 is: 4.665808\n",
            "the loss in 4750th batch / 485 is: 4.480330\n",
            "the loss in 4800th batch / 485 is: 4.712851\n",
            "the loss in 4850th batch / 485 is: 4.625083\n",
            "$$$$$$$ STARTING EPOCH 10 $$$$$$$\n",
            "the loss in 4900th batch / 485 is: 4.547584\n",
            "the loss in 4950th batch / 485 is: 4.687782\n",
            "the loss in 5000th batch / 485 is: 4.558758\n",
            "the loss in 5050th batch / 485 is: 4.672151\n",
            "the loss in 5100th batch / 485 is: 4.601034\n",
            "the loss in 5150th batch / 485 is: 4.495406\n",
            "the loss in 5200th batch / 485 is: 4.502834\n",
            "the loss in 5250th batch / 485 is: 4.446576\n",
            "the loss in 5300th batch / 485 is: 4.429564\n",
            "$$$$$$$ STARTING EPOCH 11 $$$$$$$\n",
            "the loss in 5350th batch / 485 is: 4.587529\n",
            "the loss in 5400th batch / 485 is: 4.545366\n",
            "the loss in 5450th batch / 485 is: 4.509331\n",
            "the loss in 5500th batch / 485 is: 4.452232\n",
            "the loss in 5550th batch / 485 is: 4.588824\n",
            "the loss in 5600th batch / 485 is: 4.307964\n",
            "the loss in 5650th batch / 485 is: 4.594633\n",
            "the loss in 5700th batch / 485 is: 4.475739\n",
            "the loss in 5750th batch / 485 is: 4.335704\n",
            "the loss in 5800th batch / 485 is: 4.254285\n",
            "$$$$$$$ STARTING EPOCH 12 $$$$$$$\n",
            "the loss in 5850th batch / 485 is: 4.379957\n",
            "the loss in 5900th batch / 485 is: 4.477720\n",
            "the loss in 5950th batch / 485 is: 4.367527\n",
            "the loss in 6000th batch / 485 is: 4.248518\n",
            "the loss in 6050th batch / 485 is: 4.327149\n",
            "the loss in 6100th batch / 485 is: 4.454115\n",
            "the loss in 6150th batch / 485 is: 4.287398\n",
            "the loss in 6200th batch / 485 is: 4.428172\n",
            "the loss in 6250th batch / 485 is: 4.305412\n",
            "the loss in 6300th batch / 485 is: 4.430435\n",
            "$$$$$$$ STARTING EPOCH 13 $$$$$$$\n",
            "the loss in 6350th batch / 485 is: 4.205024\n",
            "the loss in 6400th batch / 485 is: 4.272624\n",
            "the loss in 6450th batch / 485 is: 4.261752\n",
            "the loss in 6500th batch / 485 is: 4.106179\n",
            "the loss in 6550th batch / 485 is: 4.370793\n",
            "the loss in 6600th batch / 485 is: 4.404395\n",
            "the loss in 6650th batch / 485 is: 4.370373\n",
            "the loss in 6700th batch / 485 is: 4.137595\n",
            "the loss in 6750th batch / 485 is: 4.222897\n",
            "$$$$$$$ STARTING EPOCH 14 $$$$$$$\n",
            "the loss in 6800th batch / 485 is: 4.144103\n",
            "the loss in 6850th batch / 485 is: 4.270505\n",
            "the loss in 6900th batch / 485 is: 4.390914\n",
            "the loss in 6950th batch / 485 is: 4.339602\n",
            "the loss in 7000th batch / 485 is: 4.274135\n",
            "the loss in 7050th batch / 485 is: 4.225451\n",
            "the loss in 7100th batch / 485 is: 4.346615\n",
            "the loss in 7150th batch / 485 is: 4.139878\n",
            "the loss in 7200th batch / 485 is: 4.169688\n",
            "the loss in 7250th batch / 485 is: 4.307904\n",
            "$$$$$$$ STARTING EPOCH 15 $$$$$$$\n",
            "the loss in 7300th batch / 485 is: 4.268896\n",
            "the loss in 7350th batch / 485 is: 4.370116\n",
            "the loss in 7400th batch / 485 is: 4.299082\n",
            "the loss in 7450th batch / 485 is: 4.233445\n",
            "the loss in 7500th batch / 485 is: 4.334076\n",
            "the loss in 7550th batch / 485 is: 4.121541\n",
            "the loss in 7600th batch / 485 is: 4.263351\n",
            "the loss in 7650th batch / 485 is: 4.245571\n",
            "the loss in 7700th batch / 485 is: 4.545488\n",
            "the loss in 7750th batch / 485 is: 4.307586\n",
            "$$$$$$$ STARTING EPOCH 16 $$$$$$$\n",
            "the loss in 7800th batch / 485 is: 4.235422\n",
            "the loss in 7850th batch / 485 is: 4.190252\n",
            "the loss in 7900th batch / 485 is: 3.963320\n",
            "the loss in 7950th batch / 485 is: 3.769945\n",
            "the loss in 8000th batch / 485 is: 4.383160\n",
            "the loss in 8050th batch / 485 is: 4.099063\n",
            "the loss in 8100th batch / 485 is: 4.171948\n",
            "the loss in 8150th batch / 485 is: 4.047876\n",
            "the loss in 8200th batch / 485 is: 3.949920\n",
            "$$$$$$$ STARTING EPOCH 17 $$$$$$$\n",
            "the loss in 8250th batch / 485 is: 4.137502\n",
            "the loss in 8300th batch / 485 is: 4.273977\n",
            "the loss in 8350th batch / 485 is: 4.210671\n",
            "the loss in 8400th batch / 485 is: 4.093216\n",
            "the loss in 8450th batch / 485 is: 4.110765\n",
            "the loss in 8500th batch / 485 is: 4.062026\n",
            "the loss in 8550th batch / 485 is: 4.124241\n",
            "the loss in 8600th batch / 485 is: 4.169485\n",
            "the loss in 8650th batch / 485 is: 3.946532\n",
            "the loss in 8700th batch / 485 is: 4.268639\n",
            "$$$$$$$ STARTING EPOCH 18 $$$$$$$\n",
            "the loss in 8750th batch / 485 is: 4.238663\n",
            "the loss in 8800th batch / 485 is: 4.025467\n",
            "the loss in 8850th batch / 485 is: 4.101781\n",
            "the loss in 8900th batch / 485 is: 4.149718\n",
            "the loss in 8950th batch / 485 is: 4.056077\n",
            "the loss in 9000th batch / 485 is: 3.867649\n",
            "the loss in 9050th batch / 485 is: 3.949752\n",
            "the loss in 9100th batch / 485 is: 3.892493\n",
            "the loss in 9150th batch / 485 is: 4.021514\n",
            "the loss in 9200th batch / 485 is: 3.959470\n",
            "$$$$$$$ STARTING EPOCH 19 $$$$$$$\n",
            "the loss in 9250th batch / 485 is: 4.067932\n",
            "the loss in 9300th batch / 485 is: 4.221968\n",
            "the loss in 9350th batch / 485 is: 4.010835\n",
            "the loss in 9400th batch / 485 is: 4.062960\n",
            "the loss in 9450th batch / 485 is: 3.987328\n",
            "the loss in 9500th batch / 485 is: 4.040370\n",
            "the loss in 9550th batch / 485 is: 3.846929\n",
            "the loss in 9600th batch / 485 is: 3.995390\n",
            "the loss in 9650th batch / 485 is: 3.986334\n",
            "the loss in 9700th batch / 485 is: 3.991561\n",
            "$$$$$$$ STARTING EPOCH 20 $$$$$$$\n",
            "the loss in 9750th batch / 485 is: 4.208487\n",
            "the loss in 9800th batch / 485 is: 3.938820\n",
            "the loss in 9850th batch / 485 is: 3.902499\n",
            "the loss in 9900th batch / 485 is: 3.940641\n",
            "the loss in 9950th batch / 485 is: 4.064766\n",
            "the loss in 10000th batch / 485 is: 3.791245\n",
            "the loss in 10050th batch / 485 is: 3.994633\n",
            "the loss in 10100th batch / 485 is: 3.916834\n",
            "the loss in 10150th batch / 485 is: 3.770333\n",
            "$$$$$$$ STARTING EPOCH 21 $$$$$$$\n",
            "the loss in 10200th batch / 485 is: 4.005190\n",
            "the loss in 10250th batch / 485 is: 3.961098\n",
            "the loss in 10300th batch / 485 is: 3.905267\n",
            "the loss in 10350th batch / 485 is: 3.971637\n",
            "the loss in 10400th batch / 485 is: 3.878093\n",
            "the loss in 10450th batch / 485 is: 3.978882\n",
            "the loss in 10500th batch / 485 is: 3.844205\n",
            "the loss in 10550th batch / 485 is: 3.844623\n",
            "the loss in 10600th batch / 485 is: 3.991139\n",
            "the loss in 10650th batch / 485 is: 4.198211\n",
            "$$$$$$$ STARTING EPOCH 22 $$$$$$$\n",
            "the loss in 10700th batch / 485 is: 4.135542\n",
            "the loss in 10750th batch / 485 is: 3.940879\n",
            "the loss in 10800th batch / 485 is: 3.935305\n",
            "the loss in 10850th batch / 485 is: 3.981286\n",
            "the loss in 10900th batch / 485 is: 3.635698\n",
            "the loss in 10950th batch / 485 is: 4.183972\n",
            "the loss in 11000th batch / 485 is: 4.064372\n",
            "the loss in 11050th batch / 485 is: 4.031806\n",
            "the loss in 11100th batch / 485 is: 3.918888\n",
            "the loss in 11150th batch / 485 is: 3.870686\n",
            "$$$$$$$ STARTING EPOCH 23 $$$$$$$\n",
            "the loss in 11200th batch / 485 is: 3.751313\n",
            "the loss in 11250th batch / 485 is: 3.980926\n",
            "the loss in 11300th batch / 485 is: 3.755505\n",
            "the loss in 11350th batch / 485 is: 3.491118\n",
            "the loss in 11400th batch / 485 is: 3.759720\n",
            "the loss in 11450th batch / 485 is: 3.930083\n",
            "the loss in 11500th batch / 485 is: 3.954278\n",
            "the loss in 11550th batch / 485 is: 3.912371\n",
            "the loss in 11600th batch / 485 is: 3.758246\n",
            "$$$$$$$ STARTING EPOCH 24 $$$$$$$\n",
            "the loss in 11650th batch / 485 is: 4.016667\n",
            "the loss in 11700th batch / 485 is: 3.832358\n",
            "the loss in 11750th batch / 485 is: 3.819234\n",
            "the loss in 11800th batch / 485 is: 3.925170\n",
            "the loss in 11850th batch / 485 is: 3.799244\n",
            "the loss in 11900th batch / 485 is: 4.002087\n",
            "the loss in 11950th batch / 485 is: 3.973802\n",
            "the loss in 12000th batch / 485 is: 3.706003\n",
            "the loss in 12050th batch / 485 is: 3.953126\n",
            "the loss in 12100th batch / 485 is: 4.052130\n",
            "$$$$$$$ STARTING EPOCH 25 $$$$$$$\n",
            "the loss in 12150th batch / 485 is: 3.732638\n",
            "the loss in 12200th batch / 485 is: 3.535933\n",
            "the loss in 12250th batch / 485 is: 3.881989\n",
            "the loss in 12300th batch / 485 is: 3.846843\n",
            "the loss in 12350th batch / 485 is: 4.042109\n",
            "the loss in 12400th batch / 485 is: 3.828387\n",
            "the loss in 12450th batch / 485 is: 3.932912\n",
            "the loss in 12500th batch / 485 is: 3.910343\n",
            "the loss in 12550th batch / 485 is: 3.984505\n",
            "the loss in 12600th batch / 485 is: 3.944349\n",
            "$$$$$$$ STARTING EPOCH 26 $$$$$$$\n",
            "the loss in 12650th batch / 485 is: 3.928020\n",
            "the loss in 12700th batch / 485 is: 3.705267\n",
            "the loss in 12750th batch / 485 is: 3.889235\n",
            "the loss in 12800th batch / 485 is: 3.834524\n",
            "the loss in 12850th batch / 485 is: 3.790674\n",
            "the loss in 12900th batch / 485 is: 3.695943\n",
            "the loss in 12950th batch / 485 is: 3.930042\n",
            "the loss in 13000th batch / 485 is: 3.980947\n",
            "the loss in 13050th batch / 485 is: 3.965165\n",
            "$$$$$$$ STARTING EPOCH 27 $$$$$$$\n",
            "the loss in 13100th batch / 485 is: 3.983642\n",
            "the loss in 13150th batch / 485 is: 3.501255\n",
            "the loss in 13200th batch / 485 is: 3.724840\n",
            "the loss in 13250th batch / 485 is: 3.791043\n",
            "the loss in 13300th batch / 485 is: 3.705848\n",
            "the loss in 13350th batch / 485 is: 3.734001\n",
            "the loss in 13400th batch / 485 is: 3.785513\n",
            "the loss in 13450th batch / 485 is: 3.972095\n",
            "the loss in 13500th batch / 485 is: 3.741128\n",
            "the loss in 13550th batch / 485 is: 3.612266\n",
            "$$$$$$$ STARTING EPOCH 28 $$$$$$$\n",
            "the loss in 13600th batch / 485 is: 3.817696\n",
            "the loss in 13650th batch / 485 is: 3.869243\n",
            "the loss in 13700th batch / 485 is: 3.736746\n",
            "the loss in 13750th batch / 485 is: 3.662917\n",
            "the loss in 13800th batch / 485 is: 3.937301\n",
            "the loss in 13850th batch / 485 is: 3.835974\n",
            "the loss in 13900th batch / 485 is: 3.721460\n",
            "the loss in 13950th batch / 485 is: 3.686763\n",
            "the loss in 14000th batch / 485 is: 3.980683\n",
            "the loss in 14050th batch / 485 is: 3.722338\n",
            "$$$$$$$ STARTING EPOCH 29 $$$$$$$\n",
            "the loss in 14100th batch / 485 is: 3.628841\n",
            "the loss in 14150th batch / 485 is: 3.787805\n",
            "the loss in 14200th batch / 485 is: 3.641290\n",
            "the loss in 14250th batch / 485 is: 3.767367\n",
            "the loss in 14300th batch / 485 is: 3.647934\n",
            "the loss in 14350th batch / 485 is: 3.727297\n",
            "the loss in 14400th batch / 485 is: 3.684468\n",
            "the loss in 14450th batch / 485 is: 3.877566\n",
            "the loss in 14500th batch / 485 is: 3.985500\n",
            "the loss in 14550th batch / 485 is: 3.709411\n"
          ]
        }
      ],
      "source": [
        "arg_dict = {'r_click' : 0.7,\n",
        "            'r_buy' : 1,\n",
        "            'r_negative' : 1,\n",
        "            'hidden_factor' : 64,\n",
        "            'lr' : 0.0015,\n",
        "            'epoch' : 30,\n",
        "            'batch_size' : 512,\n",
        "            'neg' : 10,\n",
        "            'discount' : 0.5,\n",
        "            'smooth' : 0.0,\n",
        "            'clip' : 0.0\n",
        "            }\n",
        "losses_t = []\n",
        "SNQN_t, sess_t  = train_no_eval(sh_data_stats_t, sh_replay_buf_t, arg_dict=arg_dict, losses=losses_t, configuration='SNQN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Egos7lMeWJ_9"
      },
      "outputs": [],
      "source": [
        "predictions_t = predict_booking_batched(sess_t, SNQN_t, sh_df_test_t, sh_data_stats_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cw_dRVLWJ_9",
        "outputId": "1c602e23-dd35-4e52-bec2-cccaf27307fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.09327864321973499"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_accuracy_at(predictions_t, sh_ground_truth_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2GzCVOWJ_9"
      },
      "source": [
        "### Top popular reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-1LuAevo12Bt"
      },
      "outputs": [],
      "source": [
        "arg_dict = {'r_click' : 0.7,\n",
        "            'r_buy' : 1,\n",
        "            'r_negative' : 1,\n",
        "            'hidden_factor' : 64,\n",
        "            'lr' : 0.002,\n",
        "            'epoch' : 30,\n",
        "            'batch_size' : 512,\n",
        "            'neg' : 10,\n",
        "            'discount' : 0.5,\n",
        "            'smooth' : 0.0,\n",
        "            'clip' : 0.0\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOygo9llWJ_9",
        "outputId": "4c366ec6-88d5-48ff-fc6a-e214a425839b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/Modules/Network.py:86: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.seq = tf.compat.v1.layers.dropout(self.seq,\n",
            "/content/Modules/SASRec.py:131: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
            "/content/Modules/SASRec.py:132: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Modules/SASRec.py:133: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Modules/SASRec.py:173: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/SASRec.py:212: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Modules/SASRec.py:213: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/SASRec.py:217: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Modules/SASRec.py:218: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/Network.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Modules/Network.py:117: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$$$$$$$ STARTING EPOCH 0 $$$$$$$\n",
            "the loss in 50th batch / 486 is: 14.289396\n",
            "the loss in 100th batch / 486 is: 9.877695\n",
            "the loss in 150th batch / 486 is: 9.895224\n",
            "the loss in 200th batch / 486 is: 9.960608\n",
            "the loss in 250th batch / 486 is: 9.403543\n",
            "the loss in 300th batch / 486 is: 10.499090\n",
            "the loss in 350th batch / 486 is: 10.060200\n",
            "the loss in 400th batch / 486 is: 9.513538\n",
            "the loss in 450th batch / 486 is: 8.731632\n",
            "$$$$$$$ STARTING EPOCH 1 $$$$$$$\n",
            "the loss in 500th batch / 486 is: 8.241912\n",
            "the loss in 550th batch / 486 is: 8.178675\n",
            "the loss in 600th batch / 486 is: 7.330189\n",
            "the loss in 650th batch / 486 is: 6.978978\n",
            "the loss in 700th batch / 486 is: 7.288124\n",
            "the loss in 750th batch / 486 is: 7.113363\n",
            "the loss in 800th batch / 486 is: 6.992324\n",
            "the loss in 850th batch / 486 is: 6.775912\n",
            "the loss in 900th batch / 486 is: 6.448215\n",
            "the loss in 950th batch / 486 is: 6.499914\n",
            "$$$$$$$ STARTING EPOCH 2 $$$$$$$\n",
            "the loss in 1000th batch / 486 is: 6.484126\n",
            "the loss in 1050th batch / 486 is: 6.464953\n",
            "the loss in 1100th batch / 486 is: 6.281049\n",
            "the loss in 1150th batch / 486 is: 6.186209\n",
            "the loss in 1200th batch / 486 is: 6.179841\n",
            "the loss in 1250th batch / 486 is: 6.044618\n",
            "the loss in 1300th batch / 486 is: 6.177042\n",
            "the loss in 1350th batch / 486 is: 6.105463\n",
            "the loss in 1400th batch / 486 is: 6.063239\n",
            "the loss in 1450th batch / 486 is: 5.806714\n",
            "$$$$$$$ STARTING EPOCH 3 $$$$$$$\n",
            "the loss in 1500th batch / 486 is: 5.606690\n",
            "the loss in 1550th batch / 486 is: 5.747275\n",
            "the loss in 1600th batch / 486 is: 5.826324\n",
            "the loss in 1650th batch / 486 is: 5.621010\n",
            "the loss in 1700th batch / 486 is: 5.818447\n",
            "the loss in 1750th batch / 486 is: 5.536541\n",
            "the loss in 1800th batch / 486 is: 5.697966\n",
            "the loss in 1850th batch / 486 is: 5.331302\n",
            "the loss in 1900th batch / 486 is: 5.665789\n",
            "$$$$$$$ STARTING EPOCH 4 $$$$$$$\n",
            "the loss in 1950th batch / 486 is: 5.363151\n",
            "the loss in 2000th batch / 486 is: 5.447670\n",
            "the loss in 2050th batch / 486 is: 5.453558\n",
            "the loss in 2100th batch / 486 is: 5.394224\n",
            "the loss in 2150th batch / 486 is: 5.329993\n",
            "the loss in 2200th batch / 486 is: 5.175539\n",
            "the loss in 2250th batch / 486 is: 5.254427\n",
            "the loss in 2300th batch / 486 is: 5.338747\n",
            "the loss in 2350th batch / 486 is: 5.430101\n",
            "the loss in 2400th batch / 486 is: 5.394794\n",
            "$$$$$$$ STARTING EPOCH 5 $$$$$$$\n",
            "the loss in 2450th batch / 486 is: 5.365489\n",
            "the loss in 2500th batch / 486 is: 5.232147\n",
            "the loss in 2550th batch / 486 is: 5.298740\n",
            "the loss in 2600th batch / 486 is: 5.146144\n",
            "the loss in 2650th batch / 486 is: 5.108166\n",
            "the loss in 2700th batch / 486 is: 4.989938\n",
            "the loss in 2750th batch / 486 is: 5.123027\n",
            "the loss in 2800th batch / 486 is: 5.090201\n",
            "the loss in 2850th batch / 486 is: 4.966984\n",
            "the loss in 2900th batch / 486 is: 4.982906\n",
            "$$$$$$$ STARTING EPOCH 6 $$$$$$$\n",
            "the loss in 2950th batch / 486 is: 5.126715\n",
            "the loss in 3000th batch / 486 is: 4.820622\n",
            "the loss in 3050th batch / 486 is: 4.854991\n",
            "the loss in 3100th batch / 486 is: 5.012155\n",
            "the loss in 3150th batch / 486 is: 4.854465\n",
            "the loss in 3200th batch / 486 is: 4.786284\n",
            "the loss in 3250th batch / 486 is: 4.904774\n",
            "the loss in 3300th batch / 486 is: 4.756405\n",
            "the loss in 3350th batch / 486 is: 5.087498\n",
            "the loss in 3400th batch / 486 is: 4.796257\n",
            "$$$$$$$ STARTING EPOCH 7 $$$$$$$\n",
            "the loss in 3450th batch / 486 is: 4.988123\n",
            "the loss in 3500th batch / 486 is: 5.020288\n",
            "the loss in 3550th batch / 486 is: 4.791192\n",
            "the loss in 3600th batch / 486 is: 4.693562\n",
            "the loss in 3650th batch / 486 is: 5.123104\n",
            "the loss in 3700th batch / 486 is: 4.934083\n",
            "the loss in 3750th batch / 486 is: 4.887285\n",
            "the loss in 3800th batch / 486 is: 4.803970\n",
            "the loss in 3850th batch / 486 is: 4.832409\n",
            "$$$$$$$ STARTING EPOCH 8 $$$$$$$\n",
            "the loss in 3900th batch / 486 is: 4.938406\n",
            "the loss in 3950th batch / 486 is: 4.797094\n",
            "the loss in 4000th batch / 486 is: 4.965804\n",
            "the loss in 4050th batch / 486 is: 4.519678\n",
            "the loss in 4100th batch / 486 is: 4.830623\n",
            "the loss in 4150th batch / 486 is: 4.722149\n",
            "the loss in 4200th batch / 486 is: 4.622909\n",
            "the loss in 4250th batch / 486 is: 4.635992\n",
            "the loss in 4300th batch / 486 is: 4.894462\n",
            "the loss in 4350th batch / 486 is: 4.675998\n",
            "$$$$$$$ STARTING EPOCH 9 $$$$$$$\n",
            "the loss in 4400th batch / 486 is: 4.578084\n",
            "the loss in 4450th batch / 486 is: 4.661889\n",
            "the loss in 4500th batch / 486 is: 4.717094\n",
            "the loss in 4550th batch / 486 is: 4.668647\n",
            "the loss in 4600th batch / 486 is: 4.501318\n",
            "the loss in 4650th batch / 486 is: 4.522143\n",
            "the loss in 4700th batch / 486 is: 4.579850\n",
            "the loss in 4750th batch / 486 is: 4.581547\n",
            "the loss in 4800th batch / 486 is: 4.816068\n",
            "the loss in 4850th batch / 486 is: 4.626294\n",
            "$$$$$$$ STARTING EPOCH 10 $$$$$$$\n",
            "the loss in 4900th batch / 486 is: 4.449137\n",
            "the loss in 4950th batch / 486 is: 4.479914\n",
            "the loss in 5000th batch / 486 is: 4.425830\n",
            "the loss in 5050th batch / 486 is: 4.441042\n",
            "the loss in 5100th batch / 486 is: 4.456753\n",
            "the loss in 5150th batch / 486 is: 4.449382\n",
            "the loss in 5200th batch / 486 is: 4.455674\n",
            "the loss in 5250th batch / 486 is: 4.449806\n",
            "the loss in 5300th batch / 486 is: 4.286677\n",
            "$$$$$$$ STARTING EPOCH 11 $$$$$$$\n",
            "the loss in 5350th batch / 486 is: 4.493695\n",
            "the loss in 5400th batch / 486 is: 4.258955\n",
            "the loss in 5450th batch / 486 is: 4.189634\n",
            "the loss in 5500th batch / 486 is: 4.498987\n",
            "the loss in 5550th batch / 486 is: 4.342318\n",
            "the loss in 5600th batch / 486 is: 4.542337\n",
            "the loss in 5650th batch / 486 is: 4.349857\n",
            "the loss in 5700th batch / 486 is: 4.428373\n",
            "the loss in 5750th batch / 486 is: 4.538072\n",
            "the loss in 5800th batch / 486 is: 4.327375\n",
            "$$$$$$$ STARTING EPOCH 12 $$$$$$$\n",
            "the loss in 5850th batch / 486 is: 4.363236\n",
            "the loss in 5900th batch / 486 is: 4.378300\n",
            "the loss in 5950th batch / 486 is: 4.337890\n",
            "the loss in 6000th batch / 486 is: 4.403034\n",
            "the loss in 6050th batch / 486 is: 4.387273\n",
            "the loss in 6100th batch / 486 is: 4.441000\n",
            "the loss in 6150th batch / 486 is: 4.204257\n",
            "the loss in 6200th batch / 486 is: 4.341753\n",
            "the loss in 6250th batch / 486 is: 4.437100\n",
            "the loss in 6300th batch / 486 is: 4.049166\n",
            "$$$$$$$ STARTING EPOCH 13 $$$$$$$\n",
            "the loss in 6350th batch / 486 is: 4.178098\n",
            "the loss in 6400th batch / 486 is: 4.307460\n",
            "the loss in 6450th batch / 486 is: 4.345695\n",
            "the loss in 6500th batch / 486 is: 4.327778\n",
            "the loss in 6550th batch / 486 is: 4.336728\n",
            "the loss in 6600th batch / 486 is: 4.394205\n",
            "the loss in 6650th batch / 486 is: 4.267143\n",
            "the loss in 6700th batch / 486 is: 4.035357\n",
            "the loss in 6750th batch / 486 is: 4.413305\n",
            "the loss in 6800th batch / 486 is: 4.143991\n",
            "$$$$$$$ STARTING EPOCH 14 $$$$$$$\n",
            "the loss in 6850th batch / 486 is: 4.270591\n",
            "the loss in 6900th batch / 486 is: 4.012419\n",
            "the loss in 6950th batch / 486 is: 4.129897\n",
            "the loss in 7000th batch / 486 is: 4.480447\n",
            "the loss in 7050th batch / 486 is: 4.287185\n",
            "the loss in 7100th batch / 486 is: 4.521478\n",
            "the loss in 7150th batch / 486 is: 3.911932\n",
            "the loss in 7200th batch / 486 is: 4.093850\n",
            "the loss in 7250th batch / 486 is: 4.172448\n",
            "$$$$$$$ STARTING EPOCH 15 $$$$$$$\n",
            "the loss in 7300th batch / 486 is: 4.034969\n",
            "the loss in 7350th batch / 486 is: 4.172700\n",
            "the loss in 7400th batch / 486 is: 4.202071\n",
            "the loss in 7450th batch / 486 is: 4.141996\n",
            "the loss in 7500th batch / 486 is: 4.308278\n",
            "the loss in 7550th batch / 486 is: 4.067532\n",
            "the loss in 7600th batch / 486 is: 4.062601\n",
            "the loss in 7650th batch / 486 is: 3.930906\n",
            "the loss in 7700th batch / 486 is: 4.213921\n",
            "the loss in 7750th batch / 486 is: 3.979541\n",
            "$$$$$$$ STARTING EPOCH 16 $$$$$$$\n",
            "the loss in 7800th batch / 486 is: 4.231601\n",
            "the loss in 7850th batch / 486 is: 4.207529\n",
            "the loss in 7900th batch / 486 is: 4.231863\n",
            "the loss in 7950th batch / 486 is: 4.277642\n",
            "the loss in 8000th batch / 486 is: 4.139185\n",
            "the loss in 8050th batch / 486 is: 4.233811\n",
            "the loss in 8100th batch / 486 is: 4.088319\n",
            "the loss in 8150th batch / 486 is: 4.057300\n",
            "the loss in 8200th batch / 486 is: 4.127881\n",
            "the loss in 8250th batch / 486 is: 4.086797\n",
            "$$$$$$$ STARTING EPOCH 17 $$$$$$$\n",
            "the loss in 8300th batch / 486 is: 3.904038\n",
            "the loss in 8350th batch / 486 is: 3.977275\n",
            "the loss in 8400th batch / 486 is: 4.232657\n",
            "the loss in 8450th batch / 486 is: 4.214362\n",
            "the loss in 8500th batch / 486 is: 3.955785\n",
            "the loss in 8550th batch / 486 is: 4.077823\n",
            "the loss in 8600th batch / 486 is: 4.241000\n",
            "the loss in 8650th batch / 486 is: 4.084455\n",
            "the loss in 8700th batch / 486 is: 4.195407\n",
            "$$$$$$$ STARTING EPOCH 18 $$$$$$$\n",
            "the loss in 8750th batch / 486 is: 4.056195\n",
            "the loss in 8800th batch / 486 is: 3.994816\n",
            "the loss in 8850th batch / 486 is: 4.057446\n",
            "the loss in 8900th batch / 486 is: 4.043124\n",
            "the loss in 8950th batch / 486 is: 4.014617\n",
            "the loss in 9000th batch / 486 is: 4.012536\n",
            "the loss in 9050th batch / 486 is: 4.150935\n",
            "the loss in 9100th batch / 486 is: 4.051085\n",
            "the loss in 9150th batch / 486 is: 4.096537\n",
            "the loss in 9200th batch / 486 is: 3.830771\n",
            "$$$$$$$ STARTING EPOCH 19 $$$$$$$\n",
            "the loss in 9250th batch / 486 is: 4.035143\n",
            "the loss in 9300th batch / 486 is: 4.016966\n",
            "the loss in 9350th batch / 486 is: 3.865315\n",
            "the loss in 9400th batch / 486 is: 4.176910\n",
            "the loss in 9450th batch / 486 is: 3.986814\n",
            "the loss in 9500th batch / 486 is: 3.993670\n",
            "the loss in 9550th batch / 486 is: 4.027339\n",
            "the loss in 9600th batch / 486 is: 3.859461\n",
            "the loss in 9650th batch / 486 is: 3.995911\n",
            "the loss in 9700th batch / 486 is: 4.009061\n",
            "$$$$$$$ STARTING EPOCH 20 $$$$$$$\n",
            "the loss in 9750th batch / 486 is: 3.999306\n",
            "the loss in 9800th batch / 486 is: 3.908622\n",
            "the loss in 9850th batch / 486 is: 4.082372\n",
            "the loss in 9900th batch / 486 is: 4.225583\n",
            "the loss in 9950th batch / 486 is: 4.106395\n",
            "the loss in 10000th batch / 486 is: 3.825422\n",
            "the loss in 10050th batch / 486 is: 4.090556\n",
            "the loss in 10100th batch / 486 is: 4.105343\n",
            "the loss in 10150th batch / 486 is: 4.024823\n",
            "the loss in 10200th batch / 486 is: 3.949090\n",
            "$$$$$$$ STARTING EPOCH 21 $$$$$$$\n",
            "the loss in 10250th batch / 486 is: 4.105952\n",
            "the loss in 10300th batch / 486 is: 3.900258\n",
            "the loss in 10350th batch / 486 is: 4.152668\n",
            "the loss in 10400th batch / 486 is: 3.993617\n",
            "the loss in 10450th batch / 486 is: 3.982778\n",
            "the loss in 10500th batch / 486 is: 3.749475\n",
            "the loss in 10550th batch / 486 is: 4.003284\n",
            "the loss in 10600th batch / 486 is: 3.834981\n",
            "the loss in 10650th batch / 486 is: 3.891633\n",
            "$$$$$$$ STARTING EPOCH 22 $$$$$$$\n",
            "the loss in 10700th batch / 486 is: 3.868210\n",
            "the loss in 10750th batch / 486 is: 4.033504\n",
            "the loss in 10800th batch / 486 is: 3.957827\n",
            "the loss in 10850th batch / 486 is: 3.770982\n",
            "the loss in 10900th batch / 486 is: 3.805484\n",
            "the loss in 10950th batch / 486 is: 3.993897\n",
            "the loss in 11000th batch / 486 is: 4.062656\n",
            "the loss in 11050th batch / 486 is: 3.943520\n",
            "the loss in 11100th batch / 486 is: 3.963185\n",
            "the loss in 11150th batch / 486 is: 3.957136\n",
            "$$$$$$$ STARTING EPOCH 23 $$$$$$$\n",
            "the loss in 11200th batch / 486 is: 3.747536\n",
            "the loss in 11250th batch / 486 is: 3.874038\n",
            "the loss in 11300th batch / 486 is: 3.887060\n",
            "the loss in 11350th batch / 486 is: 3.888244\n",
            "the loss in 11400th batch / 486 is: 3.968869\n",
            "the loss in 11450th batch / 486 is: 3.982544\n",
            "the loss in 11500th batch / 486 is: 3.730579\n",
            "the loss in 11550th batch / 486 is: 3.873884\n",
            "the loss in 11600th batch / 486 is: 3.887996\n",
            "the loss in 11650th batch / 486 is: 3.994135\n",
            "$$$$$$$ STARTING EPOCH 24 $$$$$$$\n",
            "the loss in 11700th batch / 486 is: 4.018796\n",
            "the loss in 11750th batch / 486 is: 3.681269\n",
            "the loss in 11800th batch / 486 is: 3.910292\n",
            "the loss in 11850th batch / 486 is: 3.827264\n",
            "the loss in 11900th batch / 486 is: 3.913519\n",
            "the loss in 11950th batch / 486 is: 3.871175\n",
            "the loss in 12000th batch / 486 is: 3.914066\n",
            "the loss in 12050th batch / 486 is: 3.911449\n",
            "the loss in 12100th batch / 486 is: 3.863174\n",
            "the loss in 12150th batch / 486 is: 3.886809\n",
            "$$$$$$$ STARTING EPOCH 25 $$$$$$$\n",
            "the loss in 12200th batch / 486 is: 3.704209\n",
            "the loss in 12250th batch / 486 is: 3.755196\n",
            "the loss in 12300th batch / 486 is: 4.073124\n",
            "the loss in 12350th batch / 486 is: 3.735039\n",
            "the loss in 12400th batch / 486 is: 3.675719\n",
            "the loss in 12450th batch / 486 is: 3.842682\n",
            "the loss in 12500th batch / 486 is: 3.796640\n",
            "the loss in 12550th batch / 486 is: 3.799978\n",
            "the loss in 12600th batch / 486 is: 3.850403\n",
            "$$$$$$$ STARTING EPOCH 26 $$$$$$$\n",
            "the loss in 12650th batch / 486 is: 4.096255\n",
            "the loss in 12700th batch / 486 is: 3.823172\n",
            "the loss in 12750th batch / 486 is: 3.789886\n",
            "the loss in 12800th batch / 486 is: 3.853855\n",
            "the loss in 12850th batch / 486 is: 3.852757\n",
            "the loss in 12900th batch / 486 is: 3.865323\n",
            "the loss in 12950th batch / 486 is: 3.883993\n",
            "the loss in 13000th batch / 486 is: 3.826481\n",
            "the loss in 13050th batch / 486 is: 3.972410\n",
            "the loss in 13100th batch / 486 is: 3.513923\n",
            "$$$$$$$ STARTING EPOCH 27 $$$$$$$\n",
            "the loss in 13150th batch / 486 is: 3.923511\n",
            "the loss in 13200th batch / 486 is: 3.729534\n",
            "the loss in 13250th batch / 486 is: 3.767015\n",
            "the loss in 13300th batch / 486 is: 3.691189\n",
            "the loss in 13350th batch / 486 is: 3.498260\n",
            "the loss in 13400th batch / 486 is: 3.775599\n",
            "the loss in 13450th batch / 486 is: 3.896098\n",
            "the loss in 13500th batch / 486 is: 3.991264\n",
            "the loss in 13550th batch / 486 is: 3.641650\n",
            "the loss in 13600th batch / 486 is: 3.892869\n",
            "$$$$$$$ STARTING EPOCH 28 $$$$$$$\n",
            "the loss in 13650th batch / 486 is: 3.724110\n",
            "the loss in 13700th batch / 486 is: 3.691236\n",
            "the loss in 13750th batch / 486 is: 3.780725\n",
            "the loss in 13800th batch / 486 is: 3.656951\n",
            "the loss in 13850th batch / 486 is: 3.820668\n",
            "the loss in 13900th batch / 486 is: 3.946835\n",
            "the loss in 13950th batch / 486 is: 3.750750\n",
            "the loss in 14000th batch / 486 is: 3.686316\n",
            "the loss in 14050th batch / 486 is: 3.802858\n",
            "$$$$$$$ STARTING EPOCH 29 $$$$$$$\n",
            "the loss in 14100th batch / 486 is: 3.802988\n",
            "the loss in 14150th batch / 486 is: 3.604897\n",
            "the loss in 14200th batch / 486 is: 3.596960\n",
            "the loss in 14250th batch / 486 is: 3.964070\n",
            "the loss in 14300th batch / 486 is: 3.666493\n",
            "the loss in 14350th batch / 486 is: 3.675310\n",
            "the loss in 14400th batch / 486 is: 3.638602\n",
            "the loss in 14450th batch / 486 is: 3.725666\n",
            "the loss in 14500th batch / 486 is: 3.822637\n",
            "the loss in 14550th batch / 486 is: 3.714186\n"
          ]
        }
      ],
      "source": [
        "losses_p = []\n",
        "SNQN_p, sess_p  = train_no_eval(sh_data_stats_p, sh_replay_buf_p, arg_dict=arg_dict, losses=losses_p, configuration='SNQN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Wyl_JVPuWJ_9"
      },
      "outputs": [],
      "source": [
        "predictions_p = predict_booking_batched(sess_p, SNQN_p, sh_df_test_p, sh_data_stats_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKP0ExsgWJ_9",
        "outputId": "542ce6ee-e0a4-4d11-ce00-967ca632d382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.08859286836135187"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_accuracy_at(predictions_p, sh_ground_truth_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3bif5Z7WJ_9"
      },
      "source": [
        "### Most popular consecutive cities reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kj_gz3RNJquu"
      },
      "outputs": [],
      "source": [
        "arg_dict = {'r_click' : 0.5,\n",
        "            'r_buy' : 1,\n",
        "            'r_negative' : 1,\n",
        "            'hidden_factor' : 64,\n",
        "            'lr' : 0.003,\n",
        "            'epoch' : 30,\n",
        "            'batch_size' : 512,\n",
        "            'neg' : 10,\n",
        "            'discount' : 0.3,\n",
        "            'smooth' : 0.0,\n",
        "            'clip' : 0.0\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGmQy9gPWJ_9",
        "outputId": "18465d54-4608-4f4e-e7ac-1f30a0e20705"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/Modules/Network.py:86: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.seq = tf.compat.v1.layers.dropout(self.seq,\n",
            "/content/Modules/SASRec.py:131: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
            "/content/Modules/SASRec.py:132: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Modules/SASRec.py:133: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/Modules/SASRec.py:173: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/SASRec.py:212: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Modules/SASRec.py:213: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/SASRec.py:217: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/Modules/SASRec.py:218: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
            "/content/Modules/Network.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/Modules/Network.py:117: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$$$$$$$ STARTING EPOCH 0 $$$$$$$\n",
            "the loss in 50th batch / 484 is: 10.880911\n",
            "the loss in 100th batch / 484 is: 9.100770\n",
            "the loss in 150th batch / 484 is: 8.879518\n",
            "the loss in 200th batch / 484 is: 8.831940\n",
            "the loss in 250th batch / 484 is: 8.550933\n",
            "the loss in 300th batch / 484 is: 8.563575\n",
            "the loss in 350th batch / 484 is: 7.683772\n",
            "the loss in 400th batch / 484 is: 7.421700\n",
            "the loss in 450th batch / 484 is: 7.040330\n",
            "$$$$$$$ STARTING EPOCH 1 $$$$$$$\n",
            "the loss in 500th batch / 484 is: 7.061565\n",
            "the loss in 550th batch / 484 is: 7.198648\n",
            "the loss in 600th batch / 484 is: 7.013271\n",
            "the loss in 650th batch / 484 is: 6.714968\n",
            "the loss in 700th batch / 484 is: 6.407926\n",
            "the loss in 750th batch / 484 is: 6.236090\n",
            "the loss in 800th batch / 484 is: 6.268320\n",
            "the loss in 850th batch / 484 is: 6.300730\n",
            "the loss in 900th batch / 484 is: 6.212146\n",
            "the loss in 950th batch / 484 is: 5.971830\n",
            "$$$$$$$ STARTING EPOCH 2 $$$$$$$\n",
            "the loss in 1000th batch / 484 is: 5.947014\n",
            "the loss in 1050th batch / 484 is: 5.995523\n",
            "the loss in 1100th batch / 484 is: 5.967539\n",
            "the loss in 1150th batch / 484 is: 5.776017\n",
            "the loss in 1200th batch / 484 is: 5.959157\n",
            "the loss in 1250th batch / 484 is: 5.716458\n",
            "the loss in 1300th batch / 484 is: 5.772163\n",
            "the loss in 1350th batch / 484 is: 5.788449\n",
            "the loss in 1400th batch / 484 is: 5.732838\n",
            "the loss in 1450th batch / 484 is: 5.312710\n",
            "$$$$$$$ STARTING EPOCH 3 $$$$$$$\n",
            "the loss in 1500th batch / 484 is: 5.484745\n",
            "the loss in 1550th batch / 484 is: 5.665037\n",
            "the loss in 1600th batch / 484 is: 5.501987\n",
            "the loss in 1650th batch / 484 is: 5.475057\n",
            "the loss in 1700th batch / 484 is: 5.406589\n",
            "the loss in 1750th batch / 484 is: 5.318094\n",
            "the loss in 1800th batch / 484 is: 5.463829\n",
            "the loss in 1850th batch / 484 is: 5.402605\n",
            "the loss in 1900th batch / 484 is: 5.141740\n",
            "$$$$$$$ STARTING EPOCH 4 $$$$$$$\n",
            "the loss in 1950th batch / 484 is: 5.378312\n",
            "the loss in 2000th batch / 484 is: 5.279254\n",
            "the loss in 2050th batch / 484 is: 5.145211\n",
            "the loss in 2100th batch / 484 is: 5.250705\n",
            "the loss in 2150th batch / 484 is: 5.160994\n",
            "the loss in 2200th batch / 484 is: 5.148909\n",
            "the loss in 2250th batch / 484 is: 5.117205\n",
            "the loss in 2300th batch / 484 is: 5.011863\n",
            "the loss in 2350th batch / 484 is: 5.150768\n",
            "the loss in 2400th batch / 484 is: 4.914968\n",
            "$$$$$$$ STARTING EPOCH 5 $$$$$$$\n",
            "the loss in 2450th batch / 484 is: 4.830876\n",
            "the loss in 2500th batch / 484 is: 5.153777\n",
            "the loss in 2550th batch / 484 is: 5.019597\n",
            "the loss in 2600th batch / 484 is: 5.073321\n",
            "the loss in 2650th batch / 484 is: 4.795949\n",
            "the loss in 2700th batch / 484 is: 4.959063\n",
            "the loss in 2750th batch / 484 is: 4.831565\n",
            "the loss in 2800th batch / 484 is: 4.942792\n",
            "the loss in 2850th batch / 484 is: 4.877467\n",
            "the loss in 2900th batch / 484 is: 4.844432\n",
            "$$$$$$$ STARTING EPOCH 6 $$$$$$$\n",
            "the loss in 2950th batch / 484 is: 5.137732\n",
            "the loss in 3000th batch / 484 is: 4.665071\n",
            "the loss in 3050th batch / 484 is: 4.546832\n",
            "the loss in 3100th batch / 484 is: 4.917206\n",
            "the loss in 3150th batch / 484 is: 4.784290\n",
            "the loss in 3200th batch / 484 is: 4.623916\n",
            "the loss in 3250th batch / 484 is: 4.770840\n",
            "the loss in 3300th batch / 484 is: 4.615777\n",
            "the loss in 3350th batch / 484 is: 4.464267\n",
            "$$$$$$$ STARTING EPOCH 7 $$$$$$$\n",
            "the loss in 3400th batch / 484 is: 4.554678\n",
            "the loss in 3450th batch / 484 is: 4.367752\n",
            "the loss in 3500th batch / 484 is: 4.751207\n",
            "the loss in 3550th batch / 484 is: 4.458001\n",
            "the loss in 3600th batch / 484 is: 4.508776\n",
            "the loss in 3650th batch / 484 is: 4.518879\n",
            "the loss in 3700th batch / 484 is: 4.520590\n",
            "the loss in 3750th batch / 484 is: 4.484322\n",
            "the loss in 3800th batch / 484 is: 4.458503\n",
            "the loss in 3850th batch / 484 is: 4.424963\n",
            "$$$$$$$ STARTING EPOCH 8 $$$$$$$\n",
            "the loss in 3900th batch / 484 is: 4.654459\n",
            "the loss in 3950th batch / 484 is: 4.471355\n",
            "the loss in 4000th batch / 484 is: 4.408545\n",
            "the loss in 4050th batch / 484 is: 4.265324\n",
            "the loss in 4100th batch / 484 is: 4.292883\n",
            "the loss in 4150th batch / 484 is: 4.499671\n",
            "the loss in 4200th batch / 484 is: 4.456620\n",
            "the loss in 4250th batch / 484 is: 4.470326\n",
            "the loss in 4300th batch / 484 is: 4.393680\n",
            "the loss in 4350th batch / 484 is: 4.840006\n",
            "$$$$$$$ STARTING EPOCH 9 $$$$$$$\n",
            "the loss in 4400th batch / 484 is: 4.518700\n",
            "the loss in 4450th batch / 484 is: 4.503449\n",
            "the loss in 4500th batch / 484 is: 4.363199\n",
            "the loss in 4550th batch / 484 is: 4.173467\n",
            "the loss in 4600th batch / 484 is: 4.295966\n",
            "the loss in 4650th batch / 484 is: 4.239493\n",
            "the loss in 4700th batch / 484 is: 4.199254\n",
            "the loss in 4750th batch / 484 is: 4.417764\n",
            "the loss in 4800th batch / 484 is: 4.282325\n",
            "$$$$$$$ STARTING EPOCH 10 $$$$$$$\n",
            "the loss in 4850th batch / 484 is: 4.325811\n",
            "the loss in 4900th batch / 484 is: 4.137987\n",
            "the loss in 4950th batch / 484 is: 4.017894\n",
            "the loss in 5000th batch / 484 is: 4.279136\n",
            "the loss in 5050th batch / 484 is: 4.212097\n",
            "the loss in 5100th batch / 484 is: 4.367307\n",
            "the loss in 5150th batch / 484 is: 4.090802\n",
            "the loss in 5200th batch / 484 is: 4.218296\n",
            "the loss in 5250th batch / 484 is: 4.115416\n",
            "the loss in 5300th batch / 484 is: 4.081294\n",
            "$$$$$$$ STARTING EPOCH 11 $$$$$$$\n",
            "the loss in 5350th batch / 484 is: 4.299256\n",
            "the loss in 5400th batch / 484 is: 4.219320\n",
            "the loss in 5450th batch / 484 is: 4.275764\n",
            "the loss in 5500th batch / 484 is: 4.261297\n",
            "the loss in 5550th batch / 484 is: 4.279382\n",
            "the loss in 5600th batch / 484 is: 4.212774\n",
            "the loss in 5650th batch / 484 is: 4.297761\n",
            "the loss in 5700th batch / 484 is: 4.035688\n",
            "the loss in 5750th batch / 484 is: 4.319531\n",
            "the loss in 5800th batch / 484 is: 3.937134\n",
            "$$$$$$$ STARTING EPOCH 12 $$$$$$$\n",
            "the loss in 5850th batch / 484 is: 4.035923\n",
            "the loss in 5900th batch / 484 is: 4.204838\n",
            "the loss in 5950th batch / 484 is: 4.119411\n",
            "the loss in 6000th batch / 484 is: 4.001696\n",
            "the loss in 6050th batch / 484 is: 4.158131\n",
            "the loss in 6100th batch / 484 is: 4.011817\n",
            "the loss in 6150th batch / 484 is: 4.270021\n",
            "the loss in 6200th batch / 484 is: 4.425475\n",
            "the loss in 6250th batch / 484 is: 4.074686\n",
            "$$$$$$$ STARTING EPOCH 13 $$$$$$$\n",
            "the loss in 6300th batch / 484 is: 4.124556\n",
            "the loss in 6350th batch / 484 is: 4.116327\n",
            "the loss in 6400th batch / 484 is: 3.932096\n",
            "the loss in 6450th batch / 484 is: 3.973955\n",
            "the loss in 6500th batch / 484 is: 4.397202\n",
            "the loss in 6550th batch / 484 is: 3.943636\n",
            "the loss in 6600th batch / 484 is: 4.024327\n",
            "the loss in 6650th batch / 484 is: 4.137234\n",
            "the loss in 6700th batch / 484 is: 4.152223\n",
            "the loss in 6750th batch / 484 is: 3.955127\n",
            "$$$$$$$ STARTING EPOCH 14 $$$$$$$\n",
            "the loss in 6800th batch / 484 is: 3.932303\n",
            "the loss in 6850th batch / 484 is: 4.020765\n",
            "the loss in 6900th batch / 484 is: 4.051310\n",
            "the loss in 6950th batch / 484 is: 3.873406\n",
            "the loss in 7000th batch / 484 is: 4.242016\n",
            "the loss in 7050th batch / 484 is: 4.080219\n",
            "the loss in 7100th batch / 484 is: 3.914399\n",
            "the loss in 7150th batch / 484 is: 4.077003\n",
            "the loss in 7200th batch / 484 is: 4.206245\n",
            "the loss in 7250th batch / 484 is: 3.897510\n",
            "$$$$$$$ STARTING EPOCH 15 $$$$$$$\n",
            "the loss in 7300th batch / 484 is: 4.070361\n",
            "the loss in 7350th batch / 484 is: 3.938261\n",
            "the loss in 7400th batch / 484 is: 3.951207\n",
            "the loss in 7450th batch / 484 is: 4.206835\n",
            "the loss in 7500th batch / 484 is: 4.155021\n",
            "the loss in 7550th batch / 484 is: 4.018630\n",
            "the loss in 7600th batch / 484 is: 3.757236\n",
            "the loss in 7650th batch / 484 is: 3.948794\n",
            "the loss in 7700th batch / 484 is: 4.017584\n",
            "$$$$$$$ STARTING EPOCH 16 $$$$$$$\n",
            "the loss in 7750th batch / 484 is: 3.940267\n",
            "the loss in 7800th batch / 484 is: 4.098321\n",
            "the loss in 7850th batch / 484 is: 3.982319\n",
            "the loss in 7900th batch / 484 is: 4.030131\n",
            "the loss in 7950th batch / 484 is: 3.816393\n",
            "the loss in 8000th batch / 484 is: 4.164566\n",
            "the loss in 8050th batch / 484 is: 3.857127\n",
            "the loss in 8100th batch / 484 is: 4.116715\n",
            "the loss in 8150th batch / 484 is: 3.994907\n",
            "the loss in 8200th batch / 484 is: 3.897411\n",
            "$$$$$$$ STARTING EPOCH 17 $$$$$$$\n",
            "the loss in 8250th batch / 484 is: 3.984017\n",
            "the loss in 8300th batch / 484 is: 4.173186\n",
            "the loss in 8350th batch / 484 is: 3.792076\n",
            "the loss in 8400th batch / 484 is: 4.155847\n",
            "the loss in 8450th batch / 484 is: 3.918858\n",
            "the loss in 8500th batch / 484 is: 3.920357\n",
            "the loss in 8550th batch / 484 is: 3.852889\n",
            "the loss in 8600th batch / 484 is: 4.120488\n",
            "the loss in 8650th batch / 484 is: 3.871512\n",
            "the loss in 8700th batch / 484 is: 3.805977\n",
            "$$$$$$$ STARTING EPOCH 18 $$$$$$$\n",
            "the loss in 8750th batch / 484 is: 3.752194\n",
            "the loss in 8800th batch / 484 is: 3.827810\n",
            "the loss in 8850th batch / 484 is: 3.797872\n",
            "the loss in 8900th batch / 484 is: 3.777924\n",
            "the loss in 8950th batch / 484 is: 3.947150\n",
            "the loss in 9000th batch / 484 is: 3.518605\n",
            "the loss in 9050th batch / 484 is: 3.946940\n",
            "the loss in 9100th batch / 484 is: 3.760237\n",
            "the loss in 9150th batch / 484 is: 3.924003\n",
            "$$$$$$$ STARTING EPOCH 19 $$$$$$$\n",
            "the loss in 9200th batch / 484 is: 3.851591\n",
            "the loss in 9250th batch / 484 is: 3.681226\n",
            "the loss in 9300th batch / 484 is: 3.850232\n",
            "the loss in 9350th batch / 484 is: 3.925696\n",
            "the loss in 9400th batch / 484 is: 3.869180\n",
            "the loss in 9450th batch / 484 is: 3.862731\n",
            "the loss in 9500th batch / 484 is: 3.976852\n",
            "the loss in 9550th batch / 484 is: 3.862296\n",
            "the loss in 9600th batch / 484 is: 3.919704\n",
            "the loss in 9650th batch / 484 is: 3.986804\n",
            "$$$$$$$ STARTING EPOCH 20 $$$$$$$\n",
            "the loss in 9700th batch / 484 is: 3.936346\n",
            "the loss in 9750th batch / 484 is: 3.680064\n",
            "the loss in 9800th batch / 484 is: 3.691090\n",
            "the loss in 9850th batch / 484 is: 3.809626\n",
            "the loss in 9900th batch / 484 is: 3.946815\n",
            "the loss in 9950th batch / 484 is: 3.715515\n",
            "the loss in 10000th batch / 484 is: 3.951254\n",
            "the loss in 10050th batch / 484 is: 3.783992\n",
            "the loss in 10100th batch / 484 is: 3.829392\n",
            "the loss in 10150th batch / 484 is: 3.725716\n",
            "$$$$$$$ STARTING EPOCH 21 $$$$$$$\n",
            "the loss in 10200th batch / 484 is: 3.954747\n",
            "the loss in 10250th batch / 484 is: 3.917012\n",
            "the loss in 10300th batch / 484 is: 3.840605\n",
            "the loss in 10350th batch / 484 is: 3.739148\n",
            "the loss in 10400th batch / 484 is: 3.858109\n",
            "the loss in 10450th batch / 484 is: 3.818835\n",
            "the loss in 10500th batch / 484 is: 3.657115\n",
            "the loss in 10550th batch / 484 is: 3.820115\n",
            "the loss in 10600th batch / 484 is: 3.851854\n",
            "$$$$$$$ STARTING EPOCH 22 $$$$$$$\n",
            "the loss in 10650th batch / 484 is: 3.624810\n",
            "the loss in 10700th batch / 484 is: 3.797456\n",
            "the loss in 10750th batch / 484 is: 3.611327\n",
            "the loss in 10800th batch / 484 is: 3.737745\n",
            "the loss in 10850th batch / 484 is: 3.627947\n",
            "the loss in 10900th batch / 484 is: 3.690018\n",
            "the loss in 10950th batch / 484 is: 3.812954\n",
            "the loss in 11000th batch / 484 is: 3.790023\n",
            "the loss in 11050th batch / 484 is: 3.635363\n",
            "the loss in 11100th batch / 484 is: 3.555374\n",
            "$$$$$$$ STARTING EPOCH 23 $$$$$$$\n",
            "the loss in 11150th batch / 484 is: 3.697393\n",
            "the loss in 11200th batch / 484 is: 3.650083\n",
            "the loss in 11250th batch / 484 is: 3.709531\n",
            "the loss in 11300th batch / 484 is: 3.670732\n",
            "the loss in 11350th batch / 484 is: 3.581720\n",
            "the loss in 11400th batch / 484 is: 3.656057\n",
            "the loss in 11450th batch / 484 is: 3.835868\n",
            "the loss in 11500th batch / 484 is: 3.572567\n",
            "the loss in 11550th batch / 484 is: 3.665403\n",
            "the loss in 11600th batch / 484 is: 3.919152\n",
            "$$$$$$$ STARTING EPOCH 24 $$$$$$$\n",
            "the loss in 11650th batch / 484 is: 3.723017\n",
            "the loss in 11700th batch / 484 is: 3.650839\n",
            "the loss in 11750th batch / 484 is: 3.775594\n",
            "the loss in 11800th batch / 484 is: 3.734361\n",
            "the loss in 11850th batch / 484 is: 3.773779\n",
            "the loss in 11900th batch / 484 is: 3.827541\n",
            "the loss in 11950th batch / 484 is: 3.821526\n",
            "the loss in 12000th batch / 484 is: 3.578829\n",
            "the loss in 12050th batch / 484 is: 3.830488\n",
            "the loss in 12100th batch / 484 is: 3.468829\n",
            "$$$$$$$ STARTING EPOCH 25 $$$$$$$\n",
            "the loss in 12150th batch / 484 is: 3.756114\n",
            "the loss in 12200th batch / 484 is: 3.983566\n",
            "the loss in 12250th batch / 484 is: 3.737002\n",
            "the loss in 12300th batch / 484 is: 3.922154\n",
            "the loss in 12350th batch / 484 is: 3.596899\n",
            "the loss in 12400th batch / 484 is: 3.549566\n",
            "the loss in 12450th batch / 484 is: 3.591959\n",
            "the loss in 12500th batch / 484 is: 3.705704\n",
            "the loss in 12550th batch / 484 is: 3.464582\n",
            "$$$$$$$ STARTING EPOCH 26 $$$$$$$\n",
            "the loss in 12600th batch / 484 is: 3.747377\n",
            "the loss in 12650th batch / 484 is: 3.998833\n",
            "the loss in 12700th batch / 484 is: 3.693254\n",
            "the loss in 12750th batch / 484 is: 3.655801\n",
            "the loss in 12800th batch / 484 is: 3.788791\n",
            "the loss in 12850th batch / 484 is: 3.903934\n",
            "the loss in 12900th batch / 484 is: 3.622206\n",
            "the loss in 12950th batch / 484 is: 3.748017\n",
            "the loss in 13000th batch / 484 is: 3.824217\n",
            "the loss in 13050th batch / 484 is: 3.569830\n",
            "$$$$$$$ STARTING EPOCH 27 $$$$$$$\n",
            "the loss in 13100th batch / 484 is: 3.598146\n",
            "the loss in 13150th batch / 484 is: 3.370127\n",
            "the loss in 13200th batch / 484 is: 3.505386\n",
            "the loss in 13250th batch / 484 is: 3.700192\n",
            "the loss in 13300th batch / 484 is: 3.696412\n",
            "the loss in 13350th batch / 484 is: 3.475676\n",
            "the loss in 13400th batch / 484 is: 3.518044\n",
            "the loss in 13450th batch / 484 is: 3.692790\n",
            "the loss in 13500th batch / 484 is: 3.622894\n",
            "the loss in 13550th batch / 484 is: 3.509282\n",
            "$$$$$$$ STARTING EPOCH 28 $$$$$$$\n",
            "the loss in 13600th batch / 484 is: 3.728610\n",
            "the loss in 13650th batch / 484 is: 3.514228\n",
            "the loss in 13700th batch / 484 is: 3.662699\n",
            "the loss in 13750th batch / 484 is: 3.436356\n",
            "the loss in 13800th batch / 484 is: 3.931188\n",
            "the loss in 13850th batch / 484 is: 3.616488\n",
            "the loss in 13900th batch / 484 is: 3.582742\n",
            "the loss in 13950th batch / 484 is: 3.677062\n",
            "the loss in 14000th batch / 484 is: 3.352020\n",
            "$$$$$$$ STARTING EPOCH 29 $$$$$$$\n",
            "the loss in 14050th batch / 484 is: 3.779928\n",
            "the loss in 14100th batch / 484 is: 3.684394\n",
            "the loss in 14150th batch / 484 is: 3.837729\n",
            "the loss in 14200th batch / 484 is: 3.607012\n",
            "the loss in 14250th batch / 484 is: 3.510937\n",
            "the loss in 14300th batch / 484 is: 3.577364\n",
            "the loss in 14350th batch / 484 is: 3.448393\n",
            "the loss in 14400th batch / 484 is: 3.377344\n",
            "the loss in 14450th batch / 484 is: 3.568479\n",
            "the loss in 14500th batch / 484 is: 3.641794\n",
            "$$$$$$$ STARTING EPOCH 30 $$$$$$$\n",
            "the loss in 14550th batch / 484 is: 3.739359\n",
            "the loss in 14600th batch / 484 is: 3.592602\n",
            "the loss in 14650th batch / 484 is: 3.349280\n",
            "the loss in 14700th batch / 484 is: 3.574969\n",
            "the loss in 14750th batch / 484 is: 3.659404\n",
            "the loss in 14800th batch / 484 is: 3.692094\n",
            "the loss in 14850th batch / 484 is: 3.358539\n",
            "the loss in 14900th batch / 484 is: 3.589513\n",
            "the loss in 14950th batch / 484 is: 3.421650\n",
            "the loss in 15000th batch / 484 is: 3.548898\n",
            "$$$$$$$ STARTING EPOCH 31 $$$$$$$\n",
            "the loss in 15050th batch / 484 is: 3.619541\n",
            "the loss in 15100th batch / 484 is: 3.477877\n",
            "the loss in 15150th batch / 484 is: 3.578464\n",
            "the loss in 15200th batch / 484 is: 3.428988\n",
            "the loss in 15250th batch / 484 is: 3.449936\n",
            "the loss in 15300th batch / 484 is: 3.615063\n",
            "the loss in 15350th batch / 484 is: 3.305001\n",
            "the loss in 15400th batch / 484 is: 3.597098\n",
            "the loss in 15450th batch / 484 is: 3.827988\n",
            "$$$$$$$ STARTING EPOCH 32 $$$$$$$\n",
            "the loss in 15500th batch / 484 is: 3.703082\n",
            "the loss in 15550th batch / 484 is: 3.540861\n",
            "the loss in 15600th batch / 484 is: 3.491401\n",
            "the loss in 15650th batch / 484 is: 3.637630\n",
            "the loss in 15700th batch / 484 is: 3.775894\n",
            "the loss in 15750th batch / 484 is: 3.502893\n",
            "the loss in 15800th batch / 484 is: 3.510506\n",
            "the loss in 15850th batch / 484 is: 3.555846\n",
            "the loss in 15900th batch / 484 is: 3.525935\n",
            "the loss in 15950th batch / 484 is: 3.568193\n",
            "$$$$$$$ STARTING EPOCH 33 $$$$$$$\n",
            "the loss in 16000th batch / 484 is: 3.494898\n",
            "the loss in 16050th batch / 484 is: 3.694497\n",
            "the loss in 16100th batch / 484 is: 3.604552\n",
            "the loss in 16150th batch / 484 is: 3.536514\n",
            "the loss in 16200th batch / 484 is: 3.582281\n",
            "the loss in 16250th batch / 484 is: 3.370190\n",
            "the loss in 16300th batch / 484 is: 3.490824\n",
            "the loss in 16350th batch / 484 is: 3.410870\n",
            "the loss in 16400th batch / 484 is: 3.697709\n",
            "the loss in 16450th batch / 484 is: 3.513855\n",
            "$$$$$$$ STARTING EPOCH 34 $$$$$$$\n",
            "the loss in 16500th batch / 484 is: 3.372068\n",
            "the loss in 16550th batch / 484 is: 3.624506\n",
            "the loss in 16600th batch / 484 is: 3.658775\n",
            "the loss in 16650th batch / 484 is: 3.653052\n",
            "the loss in 16700th batch / 484 is: 3.498828\n",
            "the loss in 16750th batch / 484 is: 3.298248\n",
            "the loss in 16800th batch / 484 is: 3.443964\n",
            "the loss in 16850th batch / 484 is: 3.563384\n",
            "the loss in 16900th batch / 484 is: 3.539138\n",
            "$$$$$$$ STARTING EPOCH 35 $$$$$$$\n",
            "the loss in 16950th batch / 484 is: 3.617933\n",
            "the loss in 17000th batch / 484 is: 3.391044\n",
            "the loss in 17050th batch / 484 is: 3.623059\n",
            "the loss in 17100th batch / 484 is: 3.413290\n",
            "the loss in 17150th batch / 484 is: 3.342702\n",
            "the loss in 17200th batch / 484 is: 3.755098\n",
            "the loss in 17250th batch / 484 is: 3.590488\n",
            "the loss in 17300th batch / 484 is: 3.524542\n",
            "the loss in 17350th batch / 484 is: 3.607034\n",
            "the loss in 17400th batch / 484 is: 3.516922\n",
            "$$$$$$$ STARTING EPOCH 36 $$$$$$$\n",
            "the loss in 17450th batch / 484 is: 3.322793\n",
            "the loss in 17500th batch / 484 is: 3.368498\n",
            "the loss in 17550th batch / 484 is: 3.628749\n",
            "the loss in 17600th batch / 484 is: 3.592133\n",
            "the loss in 17650th batch / 484 is: 3.595021\n",
            "the loss in 17700th batch / 484 is: 3.566269\n",
            "the loss in 17750th batch / 484 is: 3.488072\n",
            "the loss in 17800th batch / 484 is: 3.633454\n",
            "the loss in 17850th batch / 484 is: 3.163358\n",
            "the loss in 17900th batch / 484 is: 3.558835\n",
            "$$$$$$$ STARTING EPOCH 37 $$$$$$$\n",
            "the loss in 17950th batch / 484 is: 3.462047\n",
            "the loss in 18000th batch / 484 is: 3.578433\n",
            "the loss in 18050th batch / 484 is: 3.452968\n",
            "the loss in 18100th batch / 484 is: 3.362734\n",
            "the loss in 18150th batch / 484 is: 3.423195\n",
            "the loss in 18200th batch / 484 is: 3.310494\n",
            "the loss in 18250th batch / 484 is: 3.527853\n",
            "the loss in 18300th batch / 484 is: 3.488871\n",
            "the loss in 18350th batch / 484 is: 3.622488\n",
            "$$$$$$$ STARTING EPOCH 38 $$$$$$$\n",
            "the loss in 18400th batch / 484 is: 3.463490\n",
            "the loss in 18450th batch / 484 is: 3.446275\n",
            "the loss in 18500th batch / 484 is: 3.601544\n",
            "the loss in 18550th batch / 484 is: 3.515841\n",
            "the loss in 18600th batch / 484 is: 3.553853\n",
            "the loss in 18650th batch / 484 is: 3.394431\n",
            "the loss in 18700th batch / 484 is: 3.537339\n",
            "the loss in 18750th batch / 484 is: 3.416707\n",
            "the loss in 18800th batch / 484 is: 3.359991\n",
            "the loss in 18850th batch / 484 is: 3.403764\n",
            "$$$$$$$ STARTING EPOCH 39 $$$$$$$\n",
            "the loss in 18900th batch / 484 is: 3.262594\n",
            "the loss in 18950th batch / 484 is: 3.428213\n",
            "the loss in 19000th batch / 484 is: 3.439519\n",
            "the loss in 19050th batch / 484 is: 3.661210\n",
            "the loss in 19100th batch / 484 is: 3.357358\n",
            "the loss in 19150th batch / 484 is: 3.294909\n",
            "the loss in 19200th batch / 484 is: 3.826860\n",
            "the loss in 19250th batch / 484 is: 3.401140\n",
            "the loss in 19300th batch / 484 is: 3.449844\n",
            "the loss in 19350th batch / 484 is: 3.433892\n",
            "$$$$$$$ STARTING EPOCH 40 $$$$$$$\n",
            "the loss in 19400th batch / 484 is: 3.466661\n",
            "the loss in 19450th batch / 484 is: 3.360152\n",
            "the loss in 19500th batch / 484 is: 3.425597\n",
            "the loss in 19550th batch / 484 is: 3.669037\n",
            "the loss in 19600th batch / 484 is: 3.482459\n",
            "the loss in 19650th batch / 484 is: 3.370394\n",
            "the loss in 19700th batch / 484 is: 3.336013\n",
            "the loss in 19750th batch / 484 is: 3.512315\n",
            "the loss in 19800th batch / 484 is: 3.463475\n",
            "$$$$$$$ STARTING EPOCH 41 $$$$$$$\n",
            "the loss in 19850th batch / 484 is: 3.294641\n",
            "the loss in 19900th batch / 484 is: 3.556571\n",
            "the loss in 19950th batch / 484 is: 3.424321\n",
            "the loss in 20000th batch / 484 is: 3.480394\n",
            "the loss in 20050th batch / 484 is: 3.316310\n",
            "the loss in 20100th batch / 484 is: 3.497054\n",
            "the loss in 20150th batch / 484 is: 3.518940\n",
            "the loss in 20200th batch / 484 is: 3.529343\n",
            "the loss in 20250th batch / 484 is: 3.332137\n",
            "the loss in 20300th batch / 484 is: 3.453114\n",
            "$$$$$$$ STARTING EPOCH 42 $$$$$$$\n",
            "the loss in 20350th batch / 484 is: 3.578210\n",
            "the loss in 20400th batch / 484 is: 3.376392\n",
            "the loss in 20450th batch / 484 is: 3.364402\n",
            "the loss in 20500th batch / 484 is: 3.525790\n",
            "the loss in 20550th batch / 484 is: 3.624294\n",
            "the loss in 20600th batch / 484 is: 3.502002\n",
            "the loss in 20650th batch / 484 is: 3.374006\n",
            "the loss in 20700th batch / 484 is: 3.494479\n",
            "the loss in 20750th batch / 484 is: 3.428015\n",
            "the loss in 20800th batch / 484 is: 3.147494\n",
            "$$$$$$$ STARTING EPOCH 43 $$$$$$$\n",
            "the loss in 20850th batch / 484 is: 3.522407\n",
            "the loss in 20900th batch / 484 is: 3.441705\n",
            "the loss in 20950th batch / 484 is: 3.539140\n",
            "the loss in 21000th batch / 484 is: 3.720140\n",
            "the loss in 21050th batch / 484 is: 3.255481\n",
            "the loss in 21100th batch / 484 is: 3.661681\n",
            "the loss in 21150th batch / 484 is: 3.338988\n",
            "the loss in 21200th batch / 484 is: 3.364893\n",
            "the loss in 21250th batch / 484 is: 3.460968\n",
            "$$$$$$$ STARTING EPOCH 44 $$$$$$$\n",
            "the loss in 21300th batch / 484 is: 3.447938\n",
            "the loss in 21350th batch / 484 is: 3.459448\n",
            "the loss in 21400th batch / 484 is: 3.683029\n",
            "the loss in 21450th batch / 484 is: 3.463881\n",
            "the loss in 21500th batch / 484 is: 3.504709\n",
            "the loss in 21550th batch / 484 is: 3.357147\n",
            "the loss in 21600th batch / 484 is: 3.590239\n",
            "the loss in 21650th batch / 484 is: 3.287905\n",
            "the loss in 21700th batch / 484 is: 3.456020\n",
            "the loss in 21750th batch / 484 is: 3.489237\n",
            "$$$$$$$ STARTING EPOCH 45 $$$$$$$\n",
            "the loss in 21800th batch / 484 is: 3.208482\n",
            "the loss in 21850th batch / 484 is: 3.498979\n",
            "the loss in 21900th batch / 484 is: 3.303333\n",
            "the loss in 21950th batch / 484 is: 3.517668\n",
            "the loss in 22000th batch / 484 is: 3.502469\n",
            "the loss in 22050th batch / 484 is: 3.419726\n",
            "the loss in 22100th batch / 484 is: 3.200071\n",
            "the loss in 22150th batch / 484 is: 3.587880\n",
            "the loss in 22200th batch / 484 is: 3.335867\n",
            "the loss in 22250th batch / 484 is: 3.319290\n",
            "$$$$$$$ STARTING EPOCH 46 $$$$$$$\n",
            "the loss in 22300th batch / 484 is: 3.389980\n",
            "the loss in 22350th batch / 484 is: 3.442029\n",
            "the loss in 22400th batch / 484 is: 3.269151\n",
            "the loss in 22450th batch / 484 is: 3.355980\n",
            "the loss in 22500th batch / 484 is: 3.521340\n",
            "the loss in 22550th batch / 484 is: 3.401558\n",
            "the loss in 22600th batch / 484 is: 3.462315\n",
            "the loss in 22650th batch / 484 is: 3.487050\n",
            "the loss in 22700th batch / 484 is: 3.356930\n",
            "$$$$$$$ STARTING EPOCH 47 $$$$$$$\n",
            "the loss in 22750th batch / 484 is: 3.587066\n",
            "the loss in 22800th batch / 484 is: 3.330658\n",
            "the loss in 22850th batch / 484 is: 3.281358\n",
            "the loss in 22900th batch / 484 is: 3.710974\n",
            "the loss in 22950th batch / 484 is: 3.350440\n",
            "the loss in 23000th batch / 484 is: 3.333766\n",
            "the loss in 23050th batch / 484 is: 3.327083\n",
            "the loss in 23100th batch / 484 is: 3.393061\n",
            "the loss in 23150th batch / 484 is: 3.376470\n",
            "the loss in 23200th batch / 484 is: 3.200243\n",
            "$$$$$$$ STARTING EPOCH 48 $$$$$$$\n",
            "the loss in 23250th batch / 484 is: 3.355267\n",
            "the loss in 23300th batch / 484 is: 3.694461\n",
            "the loss in 23350th batch / 484 is: 3.628457\n",
            "the loss in 23400th batch / 484 is: 3.226814\n",
            "the loss in 23450th batch / 484 is: 3.244580\n",
            "the loss in 23500th batch / 484 is: 3.391120\n",
            "the loss in 23550th batch / 484 is: 3.421623\n",
            "the loss in 23600th batch / 484 is: 3.493521\n",
            "the loss in 23650th batch / 484 is: 3.400689\n",
            "the loss in 23700th batch / 484 is: 3.387943\n",
            "$$$$$$$ STARTING EPOCH 49 $$$$$$$\n",
            "the loss in 23750th batch / 484 is: 3.371665\n",
            "the loss in 23800th batch / 484 is: 3.400739\n",
            "the loss in 23850th batch / 484 is: 3.561074\n",
            "the loss in 23900th batch / 484 is: 3.335821\n",
            "the loss in 23950th batch / 484 is: 3.440480\n",
            "the loss in 24000th batch / 484 is: 3.289781\n",
            "the loss in 24050th batch / 484 is: 3.531933\n",
            "the loss in 24100th batch / 484 is: 3.422039\n",
            "the loss in 24150th batch / 484 is: 3.250341\n",
            "the loss in 24200th batch / 484 is: 3.390668\n"
          ]
        }
      ],
      "source": [
        "losses_c = []\n",
        "SNQN_c, sess_c  = train_no_eval(sh_data_stats_c, sh_replay_buf_c, arg_dict=arg_dict, losses=losses_c, configuration='SNQN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iAtILxdJWJ_9"
      },
      "outputs": [],
      "source": [
        "predictions_c = predict_booking_batched(sess_c, SNQN_c, df_test_c, sh_data_stats_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX2SFhQ1WJ_9",
        "outputId": "5a4c1a29-3804-4402-fc8c-22f516f36fa9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.05124742238099387"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_accuracy_at(predictions_c, sh_ground_truth_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s630rFUYWJ_9"
      },
      "source": [
        "### SA2C with best reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beRNWhfjgJd7"
      },
      "outputs": [],
      "source": [
        "arg_dict = {'r_click' : 0.7,\n",
        "            'r_buy' : 1,\n",
        "            'r_negative' : 1,\n",
        "            'hidden_factor' : 64,\n",
        "            'lr' : 0.005,\n",
        "            'lr2' : 0.001,\n",
        "            'epoch' : 20,\n",
        "            'batch_size' : 512,\n",
        "            'neg' : 5,\n",
        "            'discount' : 0.7,\n",
        "            'smooth' : 0.0,\n",
        "            'clip' : 0.0\n",
        "            }\n",
        "losses_sa2c = []\n",
        "SA2C, sess_sa2c  = train_no_eval(data_stats_c, replay_buf_c, arg_dict=arg_dict, losses=losses_sa2c, configuration='SA2C', sa2c_switch_step=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v22oOLeb509h"
      },
      "outputs": [],
      "source": [
        "predictions_sa2c = predict_booking_batched(sess_sa2c, SA2C, df_test, data_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c71UQTEgJd7"
      },
      "outputs": [],
      "source": [
        "evaluate_accuracy_at(predictions_sa2c, ground_truth, at=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1jHB6ozWJ_-"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyUhOaJdMj1v"
      },
      "outputs": [],
      "source": [
        "topcities = df_train.item_id.value_counts().index[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8dlYx_sL9br"
      },
      "outputs": [],
      "source": [
        "test_trips = (df_test[['session_id']].drop_duplicates()).reset_index().drop('index', axis=1)\n",
        "cities_prediction = pd.DataFrame([topcities]*test_trips.shape[0], columns= [f'city_id_{i+1}' for i in range(10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4QXAc2sMJzl"
      },
      "outputs": [],
      "source": [
        "submission_toppop = pd.concat([test_trips,cities_prediction], axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMyylINLMZ3t"
      },
      "outputs": [],
      "source": [
        "submission_toppop = submission_toppop.rename({'session_id':'utrip_id'}, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg9SuVRsNE3f"
      },
      "outputs": [],
      "source": [
        "submission_toppop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vA8M8xOM6tg"
      },
      "outputs": [],
      "source": [
        "evaluate_accuracy_at(submission_toppop, ground_truth, at=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
